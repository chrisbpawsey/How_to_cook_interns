\subsection{lammps}

\begin{Verbatim}[fontsize=\stripsize]
LAMMPS <is a traditional molecular dynamics code
\end{Verbatim} 

It is also an acronym for Large-scale Atomic/Molecular Massively Parallel Simulator. It has potentials
for solid state materials which include metals and semicoductors. It also has potential for delicate matter i.e biomolecules and polymers and also
coarse-grained or mesoscopic systems.

LAMMPS is an easy tool used to model atoms or rather, as a parallel molecule test simulator at the atomic, meso, or continuum scale. This tool runs on
single processors or in parallel making the use of message-passing systems (MPI) and a spatial-decomposition of the simulator domain. A significant
number of its models have forms that give accelerated performance on CPUs, GPUs, and Intel Xeon Phi processors.

The LAMMPS example done in the getexample tool was specifically run on Magnus. The source code had been provided with a large number of atoms and their
properties and potentials. The existing SLURM file of the MPI tasks of Magnus was utilized and modified to request a total of 2 nodes. The partition
remained as debugq as it is the correct partition for Magnus. The time was changed to 20 minutes as an increase in time would be better because it gives
sufficient time to run the code. If the time is not long enough, it does not completely run the code and kills the job once the time runs out.
The two files epm2.lmp  epm2data.lmp  lammps_mpi_gnu.slurm

The following commads show the SLURM directives needed for the LAMMPS task:

\begin{tcolorbox}
\begin{verbatim}
\hyphenation{#-!-/-bin-/bash -l}
#SBATCH --job-name=hostname
#SBATCH --partition=debugq
#SBATCH --nodes=2
#SBATCH --time=00:20:00
#SBATCH --export=NONE
\end{verbatim}
\end{tcolorbox}

The program environment used for this task was GNU, hence the module was swapped from Cray to GNU and and the lammps module was loaded which is required
to run the source code and the data.

\begin{tcolorbox}
\begin{verbatim}
swap PrgEnv-cray PrgEnv-gnu
load lammps
\end{verbatim}
\end{tcolorbox}

In the template of the SLURM, an executable, a results directory and an output were declared to store the results of the LAMMPS example in the group filenames below:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
EXECUTABLE=lmp_mpi
SCRATCH=$MYSCRATCH/run_lammps/$SLURM_JOBID
RESULTS=$MYGROUP/lmp_mpi_results/$SLURM_JOBID
\end{Verbatim}
\end{tcolorbox}

Before running the example on the SCRATCH, the files ending with .lmp name which contain the data and code were copied to the SCRACTH and this was
obtained in the SLURM as:

\begin{tcolorbox}
\begin{verbatim}
cp *.lmp $SCRATCH
\end{verbatim}
\end{tcolorbox}

As this LAMMPS example is an MPI task, it was run on Magnus on 2 nodes with 24 MPI tasks per node giving a total of 48 tasks and this was implemented
with the aprun command to launch:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
\aprun -n 48 -N 24 $EXECUTABLE < epm2.lmp >> ${OUTPUT}
\end{Verbatim}
\end{tcolorbox}


The SLURM was submitted to Magnus by the identical sbatch command used in the README files of all the Magnus examples.


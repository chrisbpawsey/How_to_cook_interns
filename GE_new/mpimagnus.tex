\subsubsection{MPI Examples}

MPI is known as a Message Passing Interface application that is a communication model for moving data between processors. For MPI examples, FORTRAN and 
C codes were mainly used and ran on different environments including Cray, GNU and Intel. On Magnus, the default program environment is Cray. Therefore, 
it was necessary to load each program environment that was intended to work on by doing a:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
module swap PrgEnv-cray PrgEnv-<name of the environment>
\end{Verbatim}
\end{tcolorbox}

It is also very important to notice that if the right program environment is not loaded, the code will fail as each environment have different commands 
for compilers. However, on Magnus, swapping from one environment to the other may be very challenging for the new users. As the \emph{getexample} tool is
desired to be as automated as possible to minimize failures on tasks, it was made sure that after running each job on the Intel and GNU environments, the
environment was set back to the default environment, Cray. This prevents the users to manually list the module every time they run something on Magnus
and modify the codes to swap from one module to the other. 

The very first MPI example performed on Magnus ran on 2 nodes with a total of 48 tasks with a basic \emph{"Hello world"} source code in both C and FORTRAN. 
This example consisted of three files as mentioned previously which were README, SLURM and the source code. The SLURM script included information about
the choice in the number of nodes, the partition, the duration of the time it takes to run the job, where to run the task such as on scratch and where
to store the results, for example the group. It also specified the name of the executable, the results directory and the output file. 

For the \emph{getexample} on Magnus, the classified partition was the \emph{debugq}. Since, there were 2 nodes used, the SLURM directives were given as:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
#!/bin/bash -l
#SBATCH --job-name=GE-hostname
#SBATCH --partition=debugq
#SBATCH --nodes=2
#SBATCH --time=00:05:00
#SBATCH --export=NONE
\end{Verbatim}
\end{tcolorbox}

The generic variables such as the executable, scratch, the results directory and the output file were defined as shown below:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
EXECUTABLE=hello_mpi_cray
SCRATCH=$MYSCRATCH/run_hostname/$SLURM_JOBID
RESULTS=$MYGROUP/mpifortran_cray_results/$SLURM_JOBID

OUTPUT=mpifortran_cray.log 
\end{Verbatim}
\end{tcolorbox}

To launch the MPI job with fully occupied 2 nodes, the \emph{aprun} command was used as expressed:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
aprun -n 48 -N 24 ./$EXECUTABLE >> ${OUTPUT}
\end{Verbatim}
\end{tcolorbox}

where -n defines the total number of MPI tasks while -N specifies the number of MPI tasks per node.

In the README file, the correct program environment should be included for the codes to run correctly. For compiling on Cray, there was no need to load
this environment as the default module was Cray. However, to run on GNU and Intel, the program environment was changed from Cray to GNU or Intel as
shown below:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
module swap PrgEnv-cray PrgEnv-gnu
module swap PrgEnv-cray PrgEnv-intel
\end{Verbatim}
\end{tcolorbox}

However, the compiler commands do not change for Cray, GNU and Intel for MPI codes on Magnus, whereas they have distinct differences for other parallel
programming tasks such as OpenMP.

To compile the \emph{hello\_mpi.f90} on Cray, GNU and Intel environments:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
ftn -O2 hello_mpi.f90 -o hello_mpi_cray
\end{Verbatim}
\end{tcolorbox}

\emph{-O2} is the optimization method, while \emph{-o} placed after the source code directs to an executable called \emph{hello\_mpi\_cray} which was 
previously defined in the SLURM and chosen by the technical staff as a name. For GNU and Intel, this was called \emph{hello\_mpi\_gnu} and emph{hello\_mpi\_intel.} 

When the source code was in C, the SLURM was not different from that of FORTRAN. However, the README did change. To compile the \emph{hello\_mpi.c code}, the 
following command was used: 

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
cc -O2 hello_mpi.c -o hello_mpi_cray
\end{Verbatim}
\end{tcolorbox}

Once the codes were compiled, the SLURM was then submitted to Magnus by:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
sbatch hello_mpi_cray.slurm 
\end{Verbatim}
\end{tcolorbox}

As mentioned earlier, to prevent the users from manually listing the modules to see which modules are loaded and which module to swap from, at the end
of the SLURM and the README, the program environment was always set back to the default by:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
module swap PrgEnv-<name of the environment> PrgEnv-cray
\end{Verbatim}
\end{tcolorbox}

Another example on MPI was to run the same sources but on partially occupied nodes which means that instead of having 24 tasks per node, each node could
only have 12 tasks. When running this example on Magnus, the README remained unchanged, but there were minor changes to the SLURM.

The number of OpenMP threads was set to 1 to prevent from inadvertent OpenMP threading and the \emph{aprun} command was changed to:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
aprun -n 24 -N 12 -S 6 ./$EXECUTABLE >> ${OUTPUT}
\end{Verbatim}
\end{tcolorbox}

where -n defines the total number of MPI tasks, -N specifies the number tasks per node while -S defines the number of MPI tasks per socket.


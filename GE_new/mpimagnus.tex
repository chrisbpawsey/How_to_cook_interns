\subsubsection{MPI Examples}

MPI is known as a Message Passing Interface application that is a communication model for moving data between processors. For MPI examples, Fortran and 
c codes were mainly used and ran on different environments including Cray, GNU and Intel. On Magnus, the default program environment is Cray. Therefore, 
it was necessary to load each program environment that was intended to work on by doing a:

\begin{tcolorbox}
\begin{verbatim}
module swap PrgEnv-cray \
    PrgEnv-<name of the environment>
\end{verbatim}
\end{tcolorbox}

It is also very important to notice that if the right program environment is not loaded, the code will fail as each environment have different commands 
for compilers. However, on Magnus, swapping from one environment to the other might be very challenging for the new users. As the getexample tool is
desired to be as automated as possible to minimise failures on tasks, it was made sure that after running each job on Intel and GNU environments, the
environment was set back to the default environment, Cray. This prevents the users to manually list the module every time they run something on Magnus
and modify the codes to swap from one module to the other. 

The very first MPI example performed on Magnus ran on 2 nodes with a total of 48 tasks with a basic \emph{"Hello world"} source code in both c and Fortran. 
This example consisted of three files as mentioned previously which were README, SLURM and the source codes. The SLURM script included information about
the choice in the number of nodes, the partition, the duration of the time it takes to run the job, where to run the task such as on scratch and where
to store the results, for example the group. It also specified the name of the executable, the results directory and the output file. 

For Magnus, the classified partition is the debugq. Since, there were 2 nodes used, the SLURM directives were given as:

\begin{tcolorbox}
\begin{verbatim}
#!/bin/bash -l
#SBATCH --job-name=GE-hostname
#SBATCH --partition=debugq
#SBATCH --nodes=2
#SBATCH --time=00:05:00
#SBATCH --export=NONE
\end{verbatim}
\end{tcolorbox}

The generic variables such as the executable, scratch, the results directory and the output file were defined as:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\stripsize]
EXECUTABLE=hello_mpi_cray
SCRATCH=$MYSCRATCH/run_hostname/$SLURM_JOBID
RESULTS=$MYGROUP/mpifortran_cray_results/$SLURM_JOBID
OUTPUT=mpifortran_cray.log 
\end{Verbatim}
\end{tcolorbox}


To launch MPI job with fully occupied 2 nodes, the aprun command was used as expressed below:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\stripsize]
aprun -n 48 -N 24 ./$EXECUTABLE >> ${OUTPUT}
\end{Verbatim}
\end{tcolorbox}

where -n defines the total number of MPI tasks while -N specifies the number of MPI tasks per node as there are 2 nodes.

In the README file, the correct program environment should be included for the codes to run well. For compiling on Cray, there was no need to load
this environment as the default module was Cray. However, to run on GNU and Intel, the program environment was changed from Cray to GNU or Intel as
shown below:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\stripsize]
module swap PrgEnv-cray PrgEnv-gnu
module swap PrgEnv-cray PrgEnv-intel
\end{Verbatim}
\end{tcolorbox}

However, the compiler commands do not change for Cray, GNU and Intel for MPI codes on Magnus, whereas they have distinct differences for other parallel
programming tasks such as OpenMP.

To compile the \emph{"Hello world"} MPI Fortan code, hello\_mpi.f90 on Cray, GNU and Intel environments:

\begin{tcolorbox}
\begin{verbatim}
ftn -O2 hello_mpi.f90 -o hello_mpi_cray
\end{verbatim}
\end{tcolorbox}

-02 is the optimization method, while -o placed after the source code directs to an executable called hello\_mpi\_cray which was previously defined in
the SLURM and chosen by the user as name. For GNU and Intel, this was called hello\_mpi\_gnu and hello\_mpi\_intel. 

When the source code in use was the c code, the SLURM was not different from that of Fortran. However, the README did change. 
To compile the hello\_mpi.c code, the following command was used: 

\begin{tcolorbox}
\begin{verbatim}
cc -O2 hello_mpi.c -o hello_mpi_gnu
\end{verbatim}
\end{tcolorbox}

Once the codes were compiled, the SLURM was then submitted to Magnus by:

\begin{tcolorbox}
\begin{verbatim}
sbatch hello_mpi_cray.slurm 
\end{verbatim}
\end{tcolorbox}

As mentioned earlier, to prevent the users from manually listing the modules to see which modules are loaded and from which module to swap, at the end
of the SLURM and the README, the program environment was always set back to the default, Cray by:

\begin{tcolorbox}
\begin{verbatim}
module swap PrgEnv-<name of the environment> \
		PrgEnv-cray
\end{verbatim}
\end{tcolorbox}

Another example on MPI was to run the same sources but on partially occupied nodes which means that instead of having 24 tasks per node, each node could
only have 12 tasks. When running this example on Magnus, the README remained unchanged, but there were minor changes to the SLURM.

The number of OpenMP threads was set to 1 to prevent from inadvertent OpenMP threading and the aprun command was changed to:

\begin{tcolorbox}
\begin{verbatim}
aprun -n 24 -N 12 -S 6 ./$EXECUTABLE >> ${OUTPUT}
\end{verbatim}
\end{tcolorbox}

-n defines the total number of MPI tasks, -N specifies the number tasks per node while -S defines 6 MPI tasks per socket.


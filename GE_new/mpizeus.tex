\subsubsection{MPI Jobs}

The \emph{getexample} tool also includes MPI examples for Zeus with the GNU, Intel and PGI compilers because both Zeus and Magnus have different operating systems 
and hence, the same MPI example needs to handled differently on Zeus. It is necessary to realise that the MPI source codes used for Zeus are the same
as Magnus, but the commands to run the same source codes are different on Zeus as compared to Magnus. Additionally, when the same code is run 
with different compilers such as GNU, Intel and PGI, the compiler module for the required compiler must be loaded. These descriptions were included in
the README and SLURM files of each example within the \emph{getexample} tool and the required module for the specific compiler was already loaded within these 
files so that the user is not asked to download them manually to prevent any errors while running the code.

The two common source codes used for the MPI examples were \emph{hello\_mpi.f90} and \emph{hello\_mpi.c}. These codes were used with GNU, Intel and PGI 
compiler options on Zeus to assist the users on how to run the same code on Zeus with different modules.

As mentioned earlier, both codes were run on both Magnus and Zeus on 2 nodes. However, the partition in the SLURM was changed to the \emph{workq} from 
the \emph{debugq.} These were defined in the SLURM script of the MPI examples as shown: 

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
#SBATCH --partition=workq
#SBATCH --nodes=2
\end{Verbatim}
\end{tcolorbox}

Unlike Magnus, when --export=NONE is included in the SLURM for Zeus, the code does not compile and it gives many errors. For this reason, this line was
excluded in the SLURM files of Zeus to compile the codes correctly.

To run the code with a given executable on scratch, and store the results in a directory within the group directory to an output file, the generic 
variables were created in the SLURM as shown:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
EXECUTABLE=hello_mpi_gnu
SCRATCH=$MYSCRATCH/run_hostname/$SLURM_JOBID
RESULTS=$MYGROUP/hellompi_gnu_results_zeus/$SLURM_JOBID

OUTPUT=hello_mpi_gnu.log
\end{Verbatim}
\end{tcolorbox}

On Magnus, a total of 48 MPI tasks could be run whereas this was too much for Zeus and thus, the number of MPI tasks was reduced to 32. To run the job
on Zeus, instead of using aprun command used in Magnus, srun command was used as srun is specific to Zeus while aprun is designed for Magnus. Therefore,
to run the code, the following command was used:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
srun -n 32 -N 2 --mpi=pmi2 ./$EXECUTABLE >> ${OUTPUT}
\end{Verbatim}
\end{tcolorbox}

where -n defines the total number of MPI tasks while -N defines the number of nodes used on Zeus which is different from Magnus as -N defines the number
of MPI tasks per node. Once again, in comparison to Magnus --mpi=pmi2 was added to srun which wasn't used in aprun where --mpi=pmi2 is the MPI
implementation and must specified to srun for the correct operation.

In the README file, depending on which compiler the source code would be compiled with, the compiler module was included and loaded in the README file 
when it ran. For example, if the GNU compiler module was to be used, the gcc module was loaded with the mpt module as shown below:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
module load gcc
module load mpt
\end{Verbatim}
\end{tcolorbox}

If the code was to be compiled with Intel, the gcc module needs to be unloaded and the Intel compiler module needs to be loaded. This applies to the PGI
compiler as well, after the gcc module is unloaded, the pgi module should be loaded as shown below:

For Intel:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
module unload gcc
module load intel
module load mpt
\end{Verbatim}
\end{tcolorbox}

For PGI: 

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
module unload gcc
module load pgi
module load mpt
\end{Verbatim}
\end{tcolorbox}

The mpt module needs to be loaded whenever an MPI or OpenMP/MPI task is submitted to Zeus as it provides the SGI message to access the MPI library.

Additionally, the README file contained the command to compile the \emph{hello\_mpi.f90} code and \emph{hello\_mpi.c} code with the 
GNU or Intel compiler as shown:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
mpif90 hello_mpi.f90 -o hello_mpi_gnu
mpicc hello_mpi.c -o hello_mpi_gnu
\end{Verbatim}
\end{tcolorbox}

For MPI tasks as mentioned earlier, the command to compile the code does not change for GNU and Intel compilers, whereas it differs for PGI compiler.
Hence to compile the basic \emph{hello\_mpi.f90} and \emph{hello\_mpi.c} code, the following commands were used:

\begin{tcolorbox}
\begin{Verbatim}[fontsize=\scriptsize]
pgf90 -Mmpi=sgimpi hello_mpi.f90 -o hello_mpi_pgi
pgcc -Mmpi=sgimpi hello_mpi.c -o hello_mpi_pgi 
\end{Verbatim}
\end{tcolorbox}

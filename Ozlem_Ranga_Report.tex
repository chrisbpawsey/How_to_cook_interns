% begin the document
\begin{Document}

\Title{Internship Report for Ranganai Mapondera and Ozlem Erkilic}
\Subtitle{Getexample: Reducing Barriers to Entry on Shared HPC Resources}

\author{\IEEEauthorblockN{Ranganai Mapondera}
\IEEEauthorblockA{Pawsey Supercomputing Centre\\
Curtin University\\
Perth, Australia\\
Email:ranganai.mapondera@student.curtin.edu.au }
\and
\IEEEauthorblockN{Ozlem Erkilic}
\IEEEauthorblockA{Pawsey Supercomputing Centre\\
Curtin University\\
Perth, Australia\\
Email:ozlem.erkilic@student.curtin.edu.au}
\\


\Section{Abstract}
\begin{abstract}

The best way to understand how to compute a task based on some resources, constraints and goals on advanced computing systems is to work through sample 
codes. A lot of expert users browse through the Internet to find these examples that they can slightly modify to achieve the results they expect. 
However, this can be very diffucult for some users who are new to this environment as there are always so many of these examples which are not 
always what they specifically need. This brings out the challenge of having to modify the example to suit their requirements on a particular computing 
resource, mainly the High Performance Computing systems (HPC) since the modified examples mostly do not work due to HPC systems having different set ups.
Therefore, it is hard to tell what the problem is for inexperienced users. Quite often, the users maynot be able to tell if the example is broken or if 
the example is correct, but maybe the code requires some adjustments. This is a very common problem encountered by the users of Pawsey Supercomputing 
Centre. For this reason, this paper provides solutions to how to run and submit examples on different resources such as supercomputers of Pawsey 
Supercomputing Centre through the use of a simple tool "getexample". The "getexample" is an effective tool that is developed for the supercomputers of 
Pawsey that are dependent on a command line interface and aims to help their users learn to how to run code mainly parallel programming examples, submit 
these examples to different operating systems of Pawsey such as Magnus, Zeus and Zythos. Although, the getexample is limited to these HPC resources, it 
has the potential to be expanded to other resources and interfaces. 


\end{abstract}
\\
\Section{Introduction}
\begin{Introduction}

Even though there are many sample codes on search engines for programming purposes, it is usually very rare that these codes work at the first try 
on different high performance computing systems without modifying them. This is mainly due to each advanced computing system being built up differently
from each other to perform specific tasks for the needs of the users. At Pawsey Supercomputing Centre, there are various HPC resources where each of them
are set up with slight variations from each other. Therefore, this becomes very challanging for the users, especially the novices to change the example 
codes to display a working task on Pawsey's resources as there are many differences between each resources with many constraints. The most common 
differences between their systems are as follow:

1) Various operating systems 
2) Different program environments
3) Different compiler options with varying commands
4) Different compiler flags and wrappers for varying compiler options
5) Different module systems and paths
6) Different module naming conventions
7) Different versions of libraries
8) Different personal scratch, group and home directories
9) Different scheduling policies

For this reason, when the user runs a sample code found from the Internet for each supercomputers (Magnus, Zeus and Zythos), the code fails to compile 
successfully and hence, does not run as each individual system has specific operating system and commands set up for them. To rectify these problems, 
some HPC resources provide their users websites with working examples that aim to assist them how to carry out their tasks step by step. 
However, sometimes the commands in these examples can be outdated due to updates in compilers and the operating systems. They also can be challenging 
to follow and perform on the real systems for the users who are not very experienced with these resources. Although, one may find a well-written bash 
script to run these systems, may not know how to make this script executable by using the chmod commands. Furthermore, a user may use a sample source 
code such as a basic MPI code which includes some mpi libabraries to perform a particular job on these supercomputers but may fail when compiled due 
to these libraries being no longer valid or upgraded to a different version. Or one may complete their task successfully, but may not know in which 
filesystem to store their results, for example in scratch or their group because some supercomputers have policies on how long to keep the data on 
certain filesystems such as scratch. If they are not moved from that specific filesystem within a certain amount of time, the results get deleted and 
thus, the user loses their work. This is a major problem for the users, especially researchers and scientist since it takes very long time to collect 
their data and vital for their researches. It is also very important that these are stored correctly within these sources.

In order to minimise these problems, the getexample was developed to suplly examples which do not require further editing or modification and works
when the executable is run. Thus, it aims to teach the users of Pawsey Supercomputing Centre how to run and submit tasks on the supercomputers without 
encountering issues. The rest of this paper explains how the getexample tool works.   


\end{Introduction}
\
\
\Section{Overview of getexample}

The main aim of the "getexample" is to provide easy access to the examples.It is expected to be founded on the following terms:

1) Easy to use for everyone including beginners
2) Practical examples which can be modified or updated by other users for their own work
3) The example should be clearly and completely detailed with steps to help users to fathom and apply it

\Section{Scope}
\begin{scope}

This shows the expected results from this Internship at Pawsey Supercomputing Center
\
\



\end{scope}
\
\Section{Creating Examples}

The examples included in the getexample library were created by internship students at Pawsey with their supervisor while the source codes within the
getexample were obtained from some wiki pages and websites and were acknowledged in the examples. Some of the examples were also created due to the
needs of research students for their internship projects at Pawsey. The examples obtained in the getexample are mainly for teaching the users how to work
with parallel programming including MPI, OpenMP and hybrid jobs such as OpenMP/MPI which utilise basic source codes similar to "Hello world" program and
submit these jobs to different supercomputers such as Magnus, Zeus and Zythos using different program environments and compiler modules. 

Each individual example on the getexample occupies a directory that is reserved for them and each example consists of three files listed below:
    
1) SLURM (Simple Linux Utility for Resource Management) : This allows users to submit batch jobs to the supercomputers, check on their status and cancel 
them if needed. It contains the necessary information about the name of the executable, the results directory, the name of the output file and the jobID. 
It also allows the users to edit how many nodes the code requires to run on the HPC systems, the duration of the task that it takes, which partition to
be used and their account name. The SLURM initially creates a scracth directory for the example to run in and the results are outputted to a log file. 
Then, it creates a group directory in which the results directory is located for that example. Once the output file is completed within the 
scratch, the output file is then moved to the results directory located in the group directory and the scratch directory then gets removed.

2) Source Code : This is usually a source code in c or Fortran and taken from wiki pages to run the example.

3) README : This file is an executable bash script which can be read and run by the users. It provides information about what the source code does,
how to compile the source code depending on the program environment such as Cray, Intel, GNU or PGI, what can be modified in the SLURM directives, and 
a set of instruction on how to submit the SLURM to the supercomputers including which specific commands to use for particular supercomputers.

For the examples in the getexample tool to be user friendly, helpful and efficient for working with HPC resources, there are many design considerations 
to be held as listed below:

1) Before launching the examples to the users, it should be made sure that the examples run successfully without encountering any errors. For example,
they should be suitable for the current operating systems with the use of correct commands for the different compiler modules such as Intel, GNU, Cray
and PGI. 

2) All the files to run the example successfully should be included with the example.

3) To minimise confusion on how to perform the example on the supercomputers, the example should be executed with a single command. For example, 
if a simple shell script is used, it should be run as ./README.

4) The examples should have enough instructions about what each command does and which parts of the SLURM and the README file can be modified so that
all the files and the scripts should be able to edited easily by the users for their preferences.

5) It must be ensured that the README and SLURM have enough information about how to change certain parts of these files so that the users can run the 
example how they wish to on the supercomputers. These instructions should be understandable by the new users who are not very experienced with these
systems. For example, if the user wishes to use more nodes on the supercomputers, one should be able to know where to change it from.
 
Even though, the examples in the getexample tool are designed in a such way that once they are downloaded, the user should be able to run them without
having to modify them. However, this is not always the case as some of the supercomputers in Pawsey Supercomputing Centre such as Zythos due to
different set ups require the account name which is customized for each user and without the correct account name in the SLURM, the code fails to run. 
Therefore, the examples in the getexample tool ensures that the users are informed on how to change their account name located in the SLURM file.
Furthermore, they assist the users on how to change number of nodes used within the supercomputers and increase or reduce the number of cores used.     


\Subsection{Creating Examples on Magnus}

This is described as a Cray X40 supercomputer that consists of many nodes that are connected by a high speed network. The cray supercomputer consists of intel Xeon E5-2690 v3(Haswell) as its only processor across all compute nodes. For the compute nodes, each one of them has 2 processors and each of these 2 processors has 12 cores.
Magnus is specified to have 24 cores per node and in total these sum up to 35,712 cores across the 1488 nodes. Each of these nodes has a memory of 64GB. The maximum login nodes of magnus are 2.
During this summer internship, the focus was to provide simplified getexamples which we developed and each othe directories described below consists of mainly the source code, the slurm file and the Readme . The source code details the script with the example code taken from online sources or developed to be the job script. The jobscript produces the executable file after being compiled. The slurm file is where a batch script is submitted. It runs the executable from the jobscript. The executable file will be used by the compiler. In the slurm, we have slurm directives where we choose the number of nodes or ntasks. We also specify the compiler before the executable. The Readme file is where the executable file is run. The slurm is given because it contains the compiler. The results from running the executable are submitted to Scratch which deletes the files after some time. A new group has been created which stores all the files from scratch and in the script the results are deleted from scratch and only viewed in group. Compute nodes are attached to scratch and are capable of moving back and forth.
Fortan and C code were used for most of the examples to illustrate the differences. Some wrappers were implemented as well for these compilers which are ftn,mpif90 and cc. When using these, it is important to invoke the actual programming environment being used PrgEnv-cray, PrgEnv-gnu or PrgEnv-intel.The relevant paths are organized via the module system.



\Subsubsection{MPI Jobs}

For MPI , we used fortran and c compilers. We ran the jobs on cray, gnu and intel.


\Subsubsection{OMP Jobs}



\Subsubsection{Helloworld Jobs}



\Subsubsection{CUDA Jobs}



\Subsubsection(Hybrid Jobs}


\Subsection{Zeus}

The getexample tool also provides examples for Zeus  
According to Pawsey system descriptions Zeus is an SGI Linux cluster that is principally utilized for pre and post-preparing of information, extensive 
shared memory calculations and remote visualization work. It is diverse group with 30 nodes in different layouts.It shares the same /home,/scratch and 
/group file systems with Magnus. To access all the 30 nodes, the client has to ssh to zeus.pawsey.org.au.
Zeus contains one large-memory nodes known as Zythos, which is SGI UV2000 system. Zythos contains 24 UV blades which show up as a solitary framework 
with large amounts of shared memory of 6TB.Out of 24 blades, 20 of them contain two 6-core Intel Xeon E5-4610 CPUs and the remnant contain only one of 
these CPUs and one other Nvidia Tesla K20 GPU. Numerous clients are allowed to keep running on Zythos simultaneously. Aside from Zythos, the other 29 
nodes of Zeus all have two 8-core Intel Xeon E5-2670 CPUs. GPUs are also featured in 27 of these with the 20 containing each one of them Nvidia Tesla 
K20 GPU. Only one user is allowed to work on each of the nodes at a time. Zeus is part of the arrangement of Pawsey Project assets and it is located at 
the Pawsey Supercomputing Center and it is incorporated with the rest of the infrastructure which permits a different scope of work.
 


\Subsubsection{MPI Jobs}




\Subsubsection{OMP Jobs}



\Subsubsection{Helloworld}



\Subsubsection{Cuda Jobs}



\Subsubsection{Hybrid Jobs}


\Subsection{Zythos}



\Subsubsection{MPI Jobs}



\Subsubsection{OMP Jobs}



\Subsubsection{Helloworld}



\Subsubsection{Cuda Jobs}



\Subsubsection{Hybrid Jobs}


\Section{Other Completed Tasks}

\Subsection{Intel Xeon Processor}

\Subsection{MIC &MKL}


\Susection{Ubuntu}



\Subsection{Supercomputing Examples to Others}



\Section{Conclusion}




\Section{References}
\end{Document}
% end document

% begin the document
\documentclass[journal]{IEEEtran}
%\setcounter{secnumdepth}{2}
\begin{document}

\title{Getexample: Reducing Barriers to Entry on Shared HPC Resources}
\author{Ranganai Mapondera\\Pawsey Supercomputing Centre Curtin University Perth, Australia\\Email:ranganai.mapondera@student.curtin.edu.au}
\author{Ozlem Erkilic\\Pawsey Supercomputing Centre Curtin University\\Perth, Australia\\Email:ozlem.erkilic@student.curtin.edu.au}
\maketitle

\section{abstract}
\begin{abstract}

The best way to understand how to compute a task based on some resources, constraints and goals on advanced computing systems is to work through sample 
codes. A lot of expert users browse through the Internet to find these examples that they can slightly modify to achieve the results they expect. 
However, this can be very diffucult for some users who are new to this environment as there are always so many of these examples which are not 
always what they specifically need. This brings out the challenge of having to modify the example to suit their requirements on a particular computing 
resource, mainly the High Performance Computing systems (HPC) since the modified examples mostly do not work due to HPC systems having different set ups.
Therefore, it is hard to tell what the issue is for inexperienced users. Quite often, the users maynot be able to tell if the example is broken or if 
the example is correct, but maybe the code requires some adjustments. This is a very common issue encountered by the users of Pawsey Supercomputing 
Centre. For this reason, this paper provides solutions to how to run and submit examples on different resources such as supercomputers of Pawsey 
Supercomputing Centre through the use of a simple tool "getexample". The "getexample" is an effective tool that is developed for the supercomputers of 
Pawsey that are dependent on a command line interface and aims to help their users learn to how to run code mainly parallel programming examples, submit 
these examples to different operating systems of Pawsey such as Magnus, Zeus and Zythos. Although, the getexample is limited to these HPC resources, it 
has the potential to be expanded to other resources and interfaces. 

\end{abstract}

\section{Introduction}

\IEEEPARstart{E}{ven} though there are many sample codes on search engines for programming purposes, it is usually very rare that these codes work at 
the first try on different high performance computing systems without modifying them. This is mainly due to each advanced computing system being built 
up differently from each other to perform specific tasks for the needs of the users. At Pawsey Supercomputing Centre, there are various HPC resources 
where each of them are set up with slight variations from each other. Therefore, this becomes very challanging for the users, especially the beginners 
to change the example codes to display a working task on Pawsey's resources as there are many differences between each resources with many constraints. 
The most common differences between their systems are as follow:

1) Various operating systems 
2) Different program environments
3) Different compiler options with varying commands
4) Different compiler flags and wrappers for varying compiler options
5) Different module systems and paths
6) Different module naming conventions
7) Different versions of libraries
8) Different personal scratch, group and home directories
9) Different scheduling policies

For this reason, when the user runs a sample code found from the Internet for each supercomputers (Magnus, Zeus and Zythos), the code fails to compile 
as expected and hence, does not run as each individual system has specific operating system and commands set up for them. To rectify these problems, 
some HPC resources provide their users websites with working examples that aim to assist them how to carry out their tasks step by step. 
However, sometimes the commands in these examples can be outdated due to the updates in compilers and the operating systems. They also can be challenging 
to follow and perform on the real systems for the users who are not very experienced with these resources. Although, one may find a well-written bash 
script to run these systems, may not know how to make this script executable by changing the permissions using the chmod commands. Furthermore, a user 
may use a sample source code such as a basic MPI code which includes some mpi libabraries to perform a particular job on these supercomputers but may 
fail when compiled due to these libraries being no longer valid or upgraded to a different version. Or one may complete their task, but may 
not know in which filesystem to store their results, for example in scratch or their group because some supercomputers have policies on how long to keep 
the data on certain filesystems such as scratch. If they are not moved from that specific filesystem within a certain amount of time, the results get 
deleted and thus, the user loses their work. This is a major difficulty for the users, especially researchers and scientist since it takes very long time 
to collect their data and vital for their researches. It is also very crucial that these are stored correctly within these sources.

In order to minimise these problems, the getexample was developed to suplly examples which do not require further editing or modification and works
when the executable is run. Thus, it aims to teach the users of Pawsey Supercomputing Centre how to run and submit tasks on the supercomputers without 
encountering issues. The rest of this paper explains how the getexample tool works.   

\section{Overview of getexample}

The main aim of the "getexample" is to provide easy access to the examples for the users. For this reason, the getexample tool is expected to be have the 
following design specifications:

1) It should be easy to use for everyone including beginners. The user should be able to download the examples and run them with one command.
2) It should provide practical examples which can be modified or updated by other users for their own work. Therefore, it encourages the users to learn
how to use the resources given to them effectively.
3) The example should be clear and completely detailed with steps to help the users to fathom and apply it.

\section{Creating Examples on getexample}

The examples included in the getexample library were created by internship students at Pawsey with their supervisor while the source codes within the
getexample were obtained from some wiki pages and websites and were acknowledged in the examples. Some of the examples were also created due to the
needs of research students for their internship projects at Pawsey. The examples obtained in the getexample are mainly for teaching the users how to work
with parallel programming including MPI, OpenMP and hybrid jobs such as OpenMP/MPI which utilise basic source codes similar to "Hello world" program and
submit these jobs to different supercomputers such as Magnus, Zeus and Zythos using different program environments and compiler modules. 

Each individual example on the getexample occupies a directory that is reserved for them and each example consists of three files listed below:
    
1) SLURM (Simple Linux Utility for Resource Management) : This allows the users to submit batch jobs to the supercomputers, check on their status and 
cancel them if needed. It contains the necessary detail about the name of the executable, the results directory, the name of the output file and 
the jobID. It also allows the users to edit how many nodes the code requires to run on the HPC systems, the duration of the task that it takes, which 
partition to be used and their account name. The SLURM initially creates a scracth directory for the example to run in and the results are outputted to 
a log file. Then, it creates a group directory in which the results directory is located for that example. Once the output file is completed within the 
scratch, the output file is then moved to the results directory located in the group directory and the scratch directory then gets removed.

2) Source Code : This is usually a source code in c or Fortran and taken from wiki pages to run the example.

3) README : This file is an executable bash script which can be read and run by the users. It provides details about what the source code does,
how to compile the source code depending on the program environment such as Cray, Intel, GNU or PGI, what can be modified in the SLURM directives, and 
a set of instruction on how to submit the SLURM to the supercomputers including which specific commands to use for particular supercomputers. It can be
executed by simply typing ./README which then compiles the source code and submits the batch job to the chosen supercomputer.

For the examples in the getexample tool to be user friendly, helpful and efficient for working with HPC resources, there are many design considerations 
to be held as listed below:

1) Before introducing the examples to the users, it should be made sure that the examples run  without encountering any mistakes. For example,
they should be suitable for the current operating systems with the use of correct commands for the different compiler modules such as Intel, GNU, Cray
and PGI. 

2) All the files to run the example should be included with the example.

3) To minimise confusion on how to perform the example on the supercomputers, the example should be executed with a single command. For example, 
if a simple shell script is used, it should be run as ./README.

4) The examples should have enough instructions about what each command does and which parts of the SLURM and the README file can be modified so that
all the files and the scripts should be able to edited easily by the users for their preferences.

5) It must be ensured that the README and SLURM have enough data about how to change certain parts of these files so that the users can run the 
example how they wish to on the supercomputers. These instructions should be understandable by the new users who are not very experienced with these
systems. For example, if the user wishes to use more nodes on the supercomputers, one should be able to know where to change it from.
 
Even though, the examples in the getexample tool are designed in a such way that once they are downloaded, the user should be able to run them without
having to modify them. However, this is not always the case as some of the supercomputers in Pawsey Supercomputing Centre such as Zythos due to
different set ups require the account name which is customized for each user and without the correct account name in the SLURM, the code fails to run. 
Therefore, the examples in the getexample tool ensures that the users are informed on how to change their account name located in the SLURM file.
Furthermore, they assist the users on how to change number of nodes used within the supercomputers and increase or reduce the number of cores used.     


\subsection{Creating Examples on Magnus}

Magnus is described as a Cray X40 supercomputer that consists of many nodes that are bound by a high speed network. For the compute nodes, each one 
of them has 2 sockets and each of these has 12 cores. Magnus is also specified to have 24 cores per node and in total these sum up to 35,712 cores across 
the 1488 nodes. 

On Magnus, jobs run on the back-end of the system with the help of SLURM and ALPS (the Cray Application Level Placement Scheduler). A 
batch job is submitted to the queue system on the front-end from the sbatch command. When it runs, it executes the launch command aprun on the MOM nodes 
which are the login nodes of Magnus. The aprun keeps running on these login nodes until the application gets completed. Once aprun finishes, the SLURM 
job also completes.

As mentioned previously, the focus of the getexample tool was not only to provide working examples to the users but also assist them on how to run jobs
on their scratch directories. Therefore, whenever the batch job was submitted to Magnus, it was ran on the scracth directory and once the job was 
completed, the results were carried to the group directory. In the end, the scracth was removed. The examples used for Magnus were mainly parallel
programming examples such as MPI, OpenMP and hybrid codes which a combination of OpenMP and MPI tasks and some applications such as lammps and gromacs.
The source codes used for these examples were written in c or Fortran and were basic "Hello world" codes with the exception of the application codes.
Each example was displayed in different environments on Magnus such as GNU, Intel and more importantly the default environment, Cray with the compiler
options of ftn, mpif90 and cc. When using these environments, it was paramount to load the actual programming environment before compiling the source
codes and running them as each environment has specific compiler commands with distinct wrappers. These are explain more in detail in the following
sections.


\subsubsection{MPI Examples}

MPI is known as a Message Passing Interface application that is a communication model for moving data between processors. For MPI examples, Fortran and 
c codes were mainly used and ran on different environments including Cray, GNU and Intel. On Magnus, the default program environment is Cray. Therefore, 
it was necessary to load each program environment that was intended to work on by doing a:

module swap PrgEnv-cray PrgEnv-(name of environment). 

It is also very vital to be aware that if the right program environment is not loaded, the code will fail as each environment have different commands 
for compilers. However, on Magnus, swapping from one environment to the other might be very challenging for the new users. As the getexample tool is
desired to be as automated as possible to minimise failures on tasks, it was made sure that after running each job on Intel and GNU environments, the
environment was set back to the default environment, Cray. This prevents the users to manually list the module every time they run something on Magnus
and modify the codes to swap from one module to the other. 

The very first MPI example performed on Magnus ran on 2 nodes with a total of 48 tasks with a basic "Hello world" source code in both c and Fortran. 
This example consisted of three files as mentioned previously which were README, SLURM and the source codes. The SLURM script included information about
the choice in the number of nodes, the partition, the duration of the time it takes to run the job, where to run the task such as on scratch and where
to store the results, for example the group. It also specified the name of the executable, the results directory and the output file. 

For Magnus, the classified partition is the debugq. Since, there were 2 nodes used, the SLURM directives were given as:

#!/bin/bash -l
#SBATCH --job-name=GE-hostname
#SBATCH --partition=debugq
#SBATCH --nodes=2
#SBATCH --time=00:05:00
#SBATCH --export=NONE

The generic variables such as the executable, scratch, the results directory and the output file were define as:

EXECUTABLE=hello_mpi_cray
SCRATCH=$MYSCRATCH/run_hostname/$SLURM_JOBID
RESULTS=$MYGROUP/mpifortran_cray_results/$SLURM_JOBID

OUTPUT=mpifortran_cray.log 

To launch MPI job with fully occupied 2 nodes, the aprun command was used as expressed below:

aprun -n 48 -N 24 ./$EXECUTABLE >> ${OUTPUT}

where -n defines the total number of MPI tasks while -N specifies the number of MPI tasks per node as there are 2 nodes.

In the README file, the correct program environment should be included for the codes to run well. For compiling on Cray, there was no need to load
this environment as the default module was Cray. However, to run on GNU and Intel, the program environment was changed from Cray to GNU or Intel as
shown below:

module swap PrgEnv-cray PrgEnv-gnu
module swap PrgEnv-cray PrgEnv-intel

However, the compiler commands do not change for Cray, GNU and Intel for MPI codes on Magnus, whereas they have distinct differences for other parallel
programming tasks such as OpenMP.

To compile the "Hello world" MPI Fortan code hello_mpi.f90 on Cray, GNU and Intel environments:

ftn -O2 hello_mpi.f90 -o hello_mpi_cray

-02 is the optimization method, while -o placed after the source code directs to an executable called hello_mpi_cray which was previously defined in the
SLURM and chosen by the user as name. For GNU and Intel, this was called hello_mpi_gnu and hello_mpi_intel. 

When the source code in use was the c code, the SLURM was not different from that of Fortran. However, the README did change. To compile the hello_mpi.c 
code, the following command was used: 

cc -O2 hello_mpi.c -o hello_mpi_gnu

Once the codes were compiled, the SLURM was then submitted to Magnus by:

sbatch hello_mpi_cray.slurm 

As mentioned earlier, to prevent the users from manually listing the modules to see which modules are loaded and from which module to swap, at the end
of the SLURM and the README, the program environment was always set back to the default, Cray by:

module swap PrgEnv-(name of the environment) PrgEnv-cray

Another example on MPI was to run the same sources but on partially occupied nodes which means that instead of having 24 tasks per node, each node could
only have 12 tasks. When running this example on Magnus, the README remained unchanged, but there were minor changes to the SLURM.

The number of OpenMP threads were set to 1 to prevent from inadvertent OpenMP threading. The aprun command was changed to:

aprun -n 24 -N 12 -S 6 ./$EXECUTABLE >> ${OUTPUT}

-n defines the total number of MPI tasks, -N specifies the number tasks per node while -S defines 6 MPI tasks per socket.

\subsubsection{OMP Examples}

OpenMP (Open Multi-Processing) is known as a common shared memory model in which the threads work in parallel and access all shared memory. Therefore,
it can be thought of one task because of their restrictions to only one node. Unlike the MPI examples, the compiler commands and the wrappers do change 
for different environments on Magnus with the OpenMP tasks. Thus, it is necessary to swap to the right program environment corresponding to the compiler.

As mentioned previously, the OpenMP jobs use only one node and this node can be fully occupied node or a node occupying a single NUMA region. When it was 
fully occupied, the slurm script had a choice of only one node with the debugq partition as explained.

#SBATCH --partition=debugq
#SBATCH --nodes=1
#SBATCH --time=00:05:00
#SBATCH --export=NONE

The initiation of the job by aprun is done by specifying that there is one task and 24 threads. 
The number of threads is also called a "depth" of 24.
The OMP_NUM_THREADS should also be set to 24 giving an expression of:


export OMP_NUM_THREADS=24
aprun -n 1 -d 24 ./$EXECUTABLE >> ${OUTPUT}

For a node occupying a single NUMA region, the code are more efficient when threads are limited to one NUMA region containing 12 cores. The expression 
-d 12 ensures that threads are bound correctly and this can be clearly done by specifying this via -cc which specifies the cores used. This is all 
illustrated below as:

export OMP_NUM_THREADS=12
aprun -n 1 -d 12 -cc 0-11 ./$EXECUTABLE >> ${OUTPUT}

The source codes used for OpenMP jobs were omp_hello.f and omp_hello.c. To run these source codes on Cray, the README included the following compiling 
commands:

ftn -O2 -h omp omp_hello.f -o hello_omp_cray for fortran and
cc -O2 -h omp omp_hello.c -o hello_omp_cray for C code


Then the job was also submitted to Magnus by using the same sbatch command in the README file of the MPI examples except replacing the name of the SLURM
with the correct name for the OpenMP tasks.
It is essential to remember the module before runninIt is essential to remember the module before running the job, to load it first according$


To compile with the GNU environment, the compiler for Fortran and c codes respectively changed to 
ftn -O2 -fopenmp omp_hello.f -o omp_hello_gnu
cc -O2 -fopenmp omp_hello.c -o omp_hello_gnu
 
For the Intel environment, the compilers used were expressed as:
ftn -O2 -openmp omp_hello.f -o omp_hello_intel
cc -O2 -openmp omp_hello.c -o omp_hello_intel

\subsubsection(Hybrid Examples}

A hybrid job is a mixed job between OpenMP and MPI. It aims to take advantage of the OpenMP in NUMA region which is also known as a socket.
The OpenMP is in this case involved with one 12 core chip and runs on 6 threads on each of 8 MPI tasks which dispersed evenly between the NUMA regions.
A sum of 2 nodes is required which accomodates 8MPI tasks and 6 threads.
Besides specifying the number of nodes, the time is also shown in the slurm script as done in the previous examples.In this case we could choose 5 minutes as done in the getexamples.
The partition remains the same debugq.

#SBATCH --partition=debugq
#SBATCH --nodes=2
#SBATCH --time=00:05:00
#SBATCH --export=NONE

8MPI tasks(n) are specified to aprun to launch the job.This is specified also with 4 MPI tasks per node(N) per socket (-S 2).
Each of the 8 MPI tasks will have 6 threads (-d 6). The OMP_NUM_THREADS is also expressed as 6.
The full expression is shown as:

export OMP_NUM_THREADS=6
aprun -n 8 -N 4 -S 2 -d 6 ./code_hybrid.x

The bash script will show the compiling code as:
ftn -O2 -h omp hybrid_hello.f90 -o hello_hybrid_cray
cc -O2 -h omp hybrid_hello.c -o hello_hybrid_cray

For each of the source codes, fortran and C.

Again in this example, cray module should be loaded as:
module load PrgEnv-cray or module swap with the previously loaded module environment.

For gnu environment,for fortran and C codes respectively, the README shows the compiler code as:

ftn -O2 -fopenmp hybrid_hello.f90 -o hello_hybrid_gnu
cc -O2 -fopenmp hybrid_hello.c -o hello_hybrid_gnu

The job is submitted to Magnus by sbatch{slurm script}

The environment is then changed first before running the job:

module swap PrgEnv-cray PrgEnv-gnu

or unload cray first
module unload PrgEnv-cray
module load PrgEnv-gnu
module list

The instructions to run the executable are given in the README  file as:

./README 
This applies for all the getexamples.

In intel the compiler code changes as well to:

ftn -O2 -openmp hybrid_hello.f90 -o hello_hybrid_intel
cc -O2 -openmp hybrid_hello.c -o hello_hybrid_intel


\subsection{Zeus}

The getexample tool also provides examples for Zeus which is according to Pawsey system descriptions an SGI Linux cluster that is principally utilized 
for pre and post-preparing of data, extensive shared memory calculations and remote visualization work. It is diverse group with 30 nodes in different layouts. It shares the same /home,/scratch and 
/group file systems with Magnus. To access all the 30 nodes, the client has to ssh to zeus.pawsey.org.au.
Zeus contains one large-memory nodes known as Zythos, which is SGI UV2000 system. Zythos contains 24 UV blades which show up as a solitary framework 
with large amounts of shared memory of 6TB.Out of 24 blades, 20 of them contain two 6-core Intel Xeon E5-4610 CPUs and the remnant contain only one of 
these CPUs and one other Nvidia Tesla K20 GPU. Numerous clients are permitted to keep running on Zythos simultaneously. Aside from Zythos, the other 29 
nodes of Zeus all have two 8-core Intel Xeon E5-2670 CPUs. GPUs are also featured in 27 of these with the 20 containing each one of them Nvidia Tesla 
K20 GPU. Only one user is authorized to work on each of the nodes at a time. Zeus is part of the arrangement of Pawsey Project assets and it is located at 
the Pawsey Supercomputing Center and it is incorporated with the rest of the infrastructure which permits a different scope of work.
 


\subsubsection{MPI Jobs}

The getexample tool also includes MPI examples for Zeus with GNU, Intel and PGI compilers because both Zeus and Magnus have different operating systems 
and hence, the same MPI example needs to handled differently on Zeus. Realise that the MPI source codes used for Zeus are the same
as Magnus, but the commands to run the same source codes are different on Zeus as compared to Magnus. Additionally, when the same code is run 
with different compilers such as GNU, Intel and PGI, the compiler module for the required compiler must be loaded as Magnus. This data was
included in the README and SLURM files of each example within the getexample tool and the required module for the specific compiler was already loaded 
within these files so that the user is not asked to download them manually to prevent any mistakes while running the code.

The two common source codes used for the MPI examples were hello_mpi.f90 (a FORTRAN 90 source code) and hello_mpi.c (a c code). These codes were used 
with GNU, Intel and PGI compiler options on Zeus to assist the users on how to run the same code on Zeus with different modules, even if the command 
for compiling the same source code does not for MPI tasks on Zeus.

As mentioned earlier, both codes were run on both Magnus and Zeus on 2 nodes. However, the partition in the SLURM was changed to workq from debugq to 
workq and to run the JOB on Zeus as shown:

#SBATCH --partition=workq
#SBATCH --nodes=2

Unlike Magnus, when --export=NONE is included in the SLURM for Zeus, the code does not compile and gives many mistakes. For this reason, this line was
excluded in the SLURM files of Zeus to compile the codes correctly.

To run the code with a given executable on scratch, and store the results in a directory within the GROUP directory to an output file, the generic 
variables were created in the SLURM as shown:

EXECUTABLE=hello_mpi_gnu
SCRATCH=$MYSCRATCH/run_hostname/$SLURM_JOBID
RESULTS=$MYGROUP/hellompi_gnu_results_zeus/$SLURM_JOBID

OUTPUT=hello_mpi_gnu.log

On Magnus, a total of 48 MPI tasks could be run whereas this was too much for Zeus and thus, the number of MPI tasks was reduced to 32. To run the job
on Zeus, instead of using aprun command used in Magnus, srun command was used as srun is specific to Zeus while aprun is designed for Magnus. Therefore,
to run the code, the following command was used as:

srun -n 32 -N 2 --mpi=pmi2 ./$EXECUTABLE >> ${OUTPUT}

where -n defines the total number of MPI tasks while -N defines the number of used on Zeus which is different from Magnus as -N defines the number of
MPI tasks per node. Once again, in comparison to Magnus --mpi=pmi2 was added to srun which wasn't used in aprun where --mpi=pmi2 is the MPI
implementation and must specified to srun for the correct operation.

In the README file, depending on which compiler the source code would be compiled with, the compiler module was included and loaded in the README file 
when it ran executed. For example, if the GNU compiler module was to be used, the gcc module was loaded with the mpt module as shown below:

module load gcc
module load mpt

If the code was to be compiled with Intel, the gcc module needs to unloaded and the Intel compiler module needs to be loaded. This applies to the PGI
compiler as well, after the gcc module is unloaded, the pgi module should be loaded as shown below:

For Intel:
module unload gcc
module load intel
module load mpt

For PGI:
module unload gcc
module load pgi
module load mpt

The mpt module needs to be loaded whenever an MPI or OpenMP/MPI task is submitted to Zeus as it provides the SGI message to access the MPI library.

Additionally, the README file contains the command to compile the FORTRAN 90, hello_mpi.f90 code  and hello_mpi.c c code with the GNU or Intel compiler 
as shown:

mpif90 hello_mpi.f90 -o hello_mpi_gnu
mpicc hello_mpi.c -o hello_mpi_gnu

For MPI tasks as mentioned earlier, the command to compile the code does not change for GNU and Intel compilers, whereas it differs for PGI compiler.
Hence to compile the basic hello_mpi.f90 and hello_mpi.c code, the following commands were used:

pgf90 -Mmpi=sgimpi hello_mpi.f90 -o hello_mpi_pgi
pgcc -Mmpi=sgimpi hello_mpi.c -o hello_mpi_c 
  

\subsubsection{OMP Examples}

The OpenMP examples used for the Zeus are the same source codes as the ones used for Magnus which are omp_hello.c (c code) and omp_hello.f (FORTRAN).
However, as mentioned earlier due to the different setups between Magnus and Zeus, the SLURM and the README for these codes are different.

The main difference between the MPI and OpenMP jobs for Zeus is that the compiling command changes for all the different compiler options with OpenMP
commands as the wrappers are not the same for GNU, Intel and PGI compilers. It is vital to observe that the commands to compile hello_mpi.f90 and
hello_mpi.c were the same for GNU and Intel compilers for MPI jobs, whereas this is not the same case for OpenMP jobs anymore. However, everytime the
compiler is swapped from one to the other, the correct compiler module should be loaded because the default compiler module is gcc. If not, the system will fail to recognize the compiler commands
and will end up giving mistakes, even if the source codes and the SLURM files work.

In the getexample tool, to run an OpenMP job on Zeus only one node was used just as Magnus, but as diccussed in the MPI example, the workq partition was
used unlike the  debugq partition on Magnus. Therefore, both of the OpenMP examples run one 16-thread OpenMP instance with one node.

To run the omp_hello.f and omp_hello.c code on Zeus with GNU compiler, the number of OpenMP threads were set to 16 on the SLURM and srun command was 
used to run it as shown:

export OMP_NUM_THREADS=16
srun -n 1 -c $OMP_NUM_THREADS ./$EXECUTABLE >> ${OUTPUT}

An alternative way of running the same job could be using the omplace command.

omplace -nt $OMP_NUM_THREADS -tm open64 ./$EXECUTABLE >> ${OUTPUT}

The srun command above can be used for both Intel and PGI compilers without requiring modification. However, the omplace command would be different for
the other compilers. Observe that -tm open64 should be included above in the omplace command because when compiling with GNU, this 
command will not be identified as the default thread model is intel. Hence, -tm open64 tells that the compiler module is GNU.

Therefore, to run this job with the Intel compiler, the omplace command in the SLURM would look like:

omplace -nt $OMP_NUM_THREADS ./$EXECUTABLE >> ${OUTPUT}

To compile the omp_hello.f with the GNU compiler, the following command was used in the README file:

gfortran -O2 -fopenmp omp_hello.f -o hello_omp_gnu

The omp_hello.c code can be compiled with the GNU as shown below:

gcc -O2 -fopenmp omp_hello.c -o hello_omp_gnu

As it can be seen, the only difference between compiling the c code and the FORTRAN code is the gcc and gfortran. Both of these codes can be compiled
with Intel and PGI compilers, but the wrapper for them will different.

For Intel:
ifort -O2 -qopenmp omp_hello.f -o hello_omp_intel
icc -O2 -qopenmp omp_hello.c -o hello_omp_intel

For PGI:
pgfortran -O2 -mp omp_hello.f -o hello_omp_pgi
pgcc -mp omp_hello.c -o hello_omp_pgi


\subsubsection{Hybrid Examples}

The OpenMP/MPI hybrid codes used for Zeus were exactly same as the ones for Magnus such as hybrid_hello.f90 and hello_hybrid.c, but they both had
distinct compiler commands specific to each compiler module. Therefore, it was also necessary to include which module to be loaded in the README files
of the getexample tool.

This hybrid job requires 2 nodes and runs 1 MPI process with 16 OpenMP threads on each compiled executable. In order to launch the job to Zeus for both 
of the source codes, the number of OpenMP threads was set to 16 and the srun command was used.

export OMP_NUM_THREADS=16
srun --mpi=pmi2 -n 2 -N 2 ./$EXECUTABLE >> ${OUTPUT}

This command was used for all of the compilers without making any changes. To compile the hybrid_hello.f90 and hello.c, the following commands were used:

For GNU:
mpif90 -O2 -fopenmp hybrid_hello.f90 -o hello_hybrid_gnu
mpicc -O2 -fopenmp -O2 hello_hybrid.c -o hello_hybrid_gnu

For Intel:
mpif90 -O2 -qopenmp hybrid_hello.f90 -o hello_hybrid_intel
mpicc -O2 -qopenmp hello_hybrid.c -o hello_hybrid_intel

For PGI:
pgf90 -Mmpi=sgimpi -mp hybrid_hello.f90 -o hello_hybrid_pgi
pgcc -Mmpi=sgimpi -mp hello_hybrid.c -o hello_hybrid_pgi

\subsubsection{CUDA Examples}

The CUDA programming is a heterogeneous model where both CPU and GPU nodes are used. In CUDA, the host refers to the CPU and its memory, whereas the 
device refers to the GPU an its memory. Therefore, a CUDA code running on the host have access to the memory on the host as well as the device. It also 
executes kernel functions on the device which are executed by GPU threads in parallel. A basic CUDA works by declaring and allocating host and device 
memory. Then, it initializes the host data and transfers the data from the host to the device. Once it executes the kernel function, it transfers the 
results from the device to the host.

The getexample tool includes a basic Hello world CUDA code, hello_cuda.cu for Zeus. It uses 1 node with any generic GPU card, and this is defined in the 
SLURM file.

#SBATCH --partition=workq
#SBATCH --nodes=1
#SBATCH --gres=gpu:1

To compile the CUDA code correctly, the cuda module should be loaded and this was done in both README and the SLURM file.

module load cuda

To submit this task to Zeus, the following command was used:

./$EXECUTABLE >> ${OUTPUT}

To compile the CUDA code, the following command was used in the README file:

nvcc hello_cuda.cu -o hello_cuda_gnu
 
  
\subsection{Zythos}

Being one of the large memory nodes of Zeus, this framework is said to be an SGI UV2000 which gives access of up to 6TB of shared memory in cache-coherent non-uniform memory access (ccNUMA) mold.
The hidden equipment depends on Intel Xeon E5-4610 2.4 GHz innovation which gives 6-core nodes(*). 
Every core has about 32KB L1 direction and data reserves and one 256KB L2 memory store. 
Each of the 6-core nodes shares a 15MB L3 cache,and each node gives 128 GB main memory: 256GB per two-second, or two-node, blade.

In the system, four blades contain one Intel Xeon 6-core chip plus one Tesla K20 GPU card.
Worldwide store coherency is guaranteed by method of unique equipment which gets to physically remote memory over the system with low inertness and high data transfer capacity.
Zythos is a restricted asset and is liable to strict qualification criteria. 
One of the following criteria for every task must be met:

1) A vast detailed collection that must be held in shared memory,and more networthy than 512GB.
2) Gigantic thread-level parallelism, for example,utilizing tens or many CPU cores.

It is more preferrable that the work meets both criteria. 
On the off chance that the work does not satisfy both criteria, then Zeus is a more fitting asset.
The extensive most extreme walltime on Zythos is necessary to help investigate these expansive detailed collections.
Having programming with long runtimes or tak-level parallelism does not justify access to Zythos.
As such most of the work has to be changed the group name in order to run it. 
Specific accounts are authorized to run on Zythos. It is necessary to change account name and put the authorized account otherwise your job will not run.

Zythos has no direct access, all work is facillitated via Zeus.Login to Zeus the usual way then access Zythos in Zeus.
The advancement environment for Zythos is the as Zeus, this includes compilers, libraries,etc. 
It is therefore beneficial Zeus first before trying to work on Zythos.
Zeus is also used to submit the jobs for Zythos but the partition in SLURM changes to zythos and shown as:

#SBATCH --partition=zythos

The utilization of omplace for thread placement is identical to Zeus and detailed in the previous section.
Jobs that require message passing must make use of mpirun job launcher which is given by the SGI message passing toolkit done by making use of the shared memory mechanisms in equipment accessible on Zythos.
For better memory execution, clients are recommended to ask for assets on Zythos as entire cores, or entire CPU sets instead of letting SLURM desgnate accessible cpus sporadically on the machine. 
The basic approach to do this is by keeping -cpus-per-task at 6 at all times and conform -ntasks to suit the cpu/memory prerequisites of the job.
In so doing, SLURM will be able to allocate cpu cores successively, rather than having to pick up available cpu cores which may be scattered all over the framework.
The primary motivation behind thid is to enhance data locality and memory perfomance.

The jobs that are run on Zythos can be run on any of the three environments gnu, intel and pgi instead of cray as on Magnus.



\subsubsection{MPI Examples}

As previously done, examples are utilizing code from either c or fotran source code.
The SLURM for MPI jobs shows the request for 24 cores by asking for 4 ntasks and 6 -cpus-per-task and nodes is not. 
This means the job has access to a total of 512GB.
The final SLURM directives are detailed as:

#SBATCH --partition=zythos
#SBATCH --natasks=4
#SBATCH --cpus-per-task=6
#SBATCH --account=pawsey0001
#SBATCH --time=00:10:00

The account has to be changed to an account authorised to access Zythos.
As done on all examples, time has been specified.
The default loaded module is gcc which is for running on GNU.
To load the latest version of GNU do a module load.It is essential in this case to load mpt module otherwise the job will not run.

module load gcc
module load mpt
module list

Create an executable as done in all other examples and redirect results to MYGROUP after SCRATCH
Message passing in done by mpirun and the executable is redirected to the OUTPUT and the resulting command is written as:

mpirun -np 24 ./$EXECUTABLE >> ${OUTPUT}

The removal of SCRATCH directory is also done in SLURM file as in the default.
README or bash script is run by ./README.
It contains the command to compile the hello_mpi.f90 or hello_mpi.c code as:

mpif90 -O2 hello_mpi.f90 -o hello_mpi_gnu for fortran code or
mpicc -O2 hello_mpi.c -o hello_mpi_gnu for c code

To submit the job to Zythos:

sbatch hello_mpi_gnu.slurm

Working on Intel would require swapping of modules from gcc to intel by manually using commands:

module swap gcc intel
module load mpt
module list

All the commands on the SLURM and README remain the same because of the use of wrappers: mpif90 and mpicc for fortran and c code respectively.

For pgi environment, the READE compiles with a different code shown as:

pgf90 -Mmpi=sgimpi hello_mpi.f90 -o hello_mpi_pgi for fortran code and 
pgcc -Mmpi=sgimpi hello_mpi.c -o hello_mpi_pgi for c code

Submit the job to Zythos :
sbatch hello_mpi_pgi.slurm

The SLURM does not change much except of the names of OUTPUT and results directory.
The message is passed through mpirun for all examples on Zythos.
Before running the job, load the pgi module and other necessary modules by swapping from intel.

module swap intel pgi
module load mpt
module list

Run the job ./README and use jobID to view results in $MYGROUP directory.


\subsubsection{OMP Examples}

The SLURM script for OpeMP requests for 2 nodes and 6 cores which is a total of 12 cores. 
The total memory carried is about 128GB in each of them.
We specify the 2 nodes as ntasks rather than nodes in the SLURM directives.

#SBATCH --partition=zythos
#SBATCH --ntasks=2
#SBATCH --cpus-per-task=6
#SBATCH --account=pawsey0001
#SBATCH --time=00:10:00

As usual, the OUTPUT name should be declared as a log file
OUTPUT=hello_omp_gnu.log

Omplace is used in OpenMP to control thread placement with a default of about 6 threads per node. 
This gives a total of 12 threads in this case since 2 nodes are being utilized.
The commands as written in the SLURM  is:

export OMP_NUM_THREADS=12
omplace -nt $OMP_NUM_THREADS ./$EXECUTABLE >> ${OUTPUT}

To compile the omp_hello.f90 for fortran with GNU we use:
gfortran -O2 -fopenmp omp_hello.f90 -o hello_omp_gnu

To compile the omp_hello.c code with GNU we use:
gcc -O2 -fopen omp_hello.c -o hello_omp_gnu

and to submit the job to Zythos
sbatch hello_omp_gnu.slurm

Although gcc is the default, it is best to load the module and other necessary modules manually and use the latest version of gcc.

module load gcc
module load mpt
module list
 
Intel compiler is different from GNU and for fortran it is written as:

ifort -O2 -qopenmp omp_hello.f90 -o hello_omp_intel

The c code compiler in the bash script is written as:

mpicc -O2 hello_mpi.c -o hello_mpi_intel

The sbatch{SLURM} is done to submit the job to Zythos

To compile the OpenMP fortran code with PGI we use the commands:

pgfortran -O2 -mp omp_hello.f90 -o hello_omp_pgi

To compile the OpenMP c code with PGI  use:

pgcc -mp omp_hello.c -o hello_omp_pgi

Before running the README, it is vital to swap the module and load the required modules for PGI

module swap intel pgi
module load mpt
module list


\subsubsection{Cuda Examples}

\subsubsection{Hybrid Examples}

To characteristic approach to run hybrid jobs on Zythos is by running one MPI task for each 6-core NUMA region.
The job script for MPI contains a request for 4 nodes and begins 4 MPI tasks by using mpirun which is combined with omplace to place threads.

#SBATCH --partition=zythos
#SBATCH --account=pawsey0001
#SBATCH --ntasks=4
#SBATCH --cpus-per-task=6
#SBATCH --time=00:10:00

The number of threads are given as well as passing the message through mpirun as:

export OMP_NUM_THREADS=6
mpirun -np 4 omplace -nt $OMP_NUM_THREADS ./$EXECUTABLE >> ${OUTPUT}

These commands remain the same for all the SLURM scripts for Hybrid jobs on Zythos.

for GNU to compile the hybrid codes:

mpif90 -O2 -fopenmp hybrid_hello.f90 -o hello_hybrid_gnu
gcc -fopenmp omp_hello.c -o hello_omp_gnu

GNU must be loaded
 
module load gcc
module load mpt
module list

\section{Applications of Getexample}

\subsection{LAMMPS}

LAMMPS is a traditional molecular dynamics code. It is also an acronym for Large-scale Atomic/Molecular Massively Parallel Simulator.
It has potentials for solid state materials which includes metals and semicoductors. It also has potential for delicate matter i.e biomolecules and polymers and also coarse-grained or mesoscopic systems.
LAMMPS is an easy tool used to model atoms or rather, as a parallel molecule test simulator at the atomic, meso, or continuum scale.
This tool runs on single processors or in parallel making use of message-passing systems and a apatial-decomposition of the simulator domain.
Asignificant number of its models have forms that give accelerated performance on CPUs, GPUs, and Intel Xeon Phis.
This code is intended to be anything but difficult to allow for modifications and stretch out for new functions.
LAMMPS code is circulated around as an open source code under the terms of the GPL. It is easily downloaded from the internet.

The LAMMPS example done on getexample was specifically run on Magnus.
The source code had been provided with a large number of atoms and their properties and potentials.
A SLURM file was successfuly modified and utilized which contains the following SLURM directives.
A total of 2 nodes was requested, partition remains as debugq on Magnus. 
Time can be changed but 20 minutes proved to be the most effective. 
An increase would be better because it will give it sufficient time to run the code. 
A decrease in time does not allow the code to run and be executed as the source code contains a lot of atoms.

%\usepackage{graphicx}
%\graphicspath{ {/Desktop/LAMMPS/} }

%\begin{document}
This is SLURM script for LAMMPS 

%\includegraphics{LAMMPS}
%\end{document}

#!/bin/bash -l
#SBATCH --job-name=hostname
#SBATCH --partition=debugq
#SBATCH --nodes=2
#SBATCH --time=00:20:00
#SBATCH --export=NONE

The compiler used was GNU. We changed the currenty loaded to GNU and an additional module is added lammps which is the most important to run this source code.

module swap PrgEnv-cray PrgEnv-gnu
module load lammps
module list

As in the template of SLURM an EXECUTABLE was given and the OUTPUT.log name was declared.
A code was added which will be run in the SCRATCH directory which copies the EXECUTABLE to SCRATCH first. It is written as:

cp *.lmp $SCRATCH

Message-passing was done using aprun as on previous Magnus examples.
We launched the job by specifying to aprun 48 MPI tasks (-n 48) with 24 tasks per node (-N 24).
This is shown as:

aprun -n 48 -N 24 $EXECUTABLE < epm2.lmp >> ${OUTPUT}
 
If the number of nodes changes, the resulting number of  MPI tasks is a multiplication of the nodes with the number of tasks per node which is 24.
The rest of the SLURM file codes do not change as on the template.
The README contains the commands to submit the job to Magnus as:

sbatch lammps_mpi_gnu


\subsection{Gromacs}





\subsection{Intel Xeon Processor}

\subsubsection{MIC &MKL}  


\section{Maintanence and Future Work}

Even though the getexample tool is designed for the latest operating systems of the Pawsey Supercomputing Centre to show the users how to use
Pawsey's HPC resources and perfom particular tasks on them, there will be a time where the getexample tool will encounter problems due 
to the changes and updates made to the advanced computing resources. This means that a previously working example may fail to work and resulting 
inconvenience for the users. In order to prevent the getexample becoming outdated and losing the users' trust, it is the technical stuff's 
duty to make sure that the getexample works which requires them to check for updates regularly, change the files in the getexample if necessary and 
modify the codes to suit the current systems.

However, the getexample tool is a large library with many examples. Thus, it can be really frustruating and hard for the system administrators to run 
all the examples one by one and having to fix these problems. In order to make this procedure easier for them, a basic bash executable script can be 
used which runs all the examples at once, checks their status and prints out which example was succesful or went wrong to an output file based on their
status.

The following bash script is a suggestion on how to run all the examples at once and test status of the examples. This script only test three examples
from the getexample for Magnus which are the "Hello world" Fortran examples with GNU, Intel and Cray environments. It runs on the home directory of the
user and changes the directory to the Magnus examples within the getexample library. It then, uses the cd command to change into the subdirectories of 
Magnus in which these three examples are located. Every time, the script comes across the README file, it runs this file by using the ./README command
to submit these examples to Magnus.

#!/bin/bash
# This script tests if all codes for Magnus work.

# Look for magnus_results.log
# Remove this file initially to create it each time
# the program runs.
rm magnus_results.log  

# Change the directory to Magnus
cd getexample/magnus_examples/

DIRECTORY=("fortran_helloworld_cray" 
           "fortran_helloworld_intel" 
	   "fortran_helloworld_gnu")

tLen=${#DIRECTORY[@]}

for ((i=0; i<${tLen}; i++))

do

cd ${DIRECTORY[$i]}
./README

cd ..

done

In order to check the status of each example, the following code is added to the SLURM files of the examples after the aprun command. This simply checks
whether the job returns 0 or 1 and if the status does not return 0, the example fails to run and hence prints the name of the example's executable with
its jobID and tells not run to a second output file. If the job returns 1, the example runs well and prints the same thing except with printing out the opposite instead.

aprun -n 1 ./$EXECUTABLE >> ${OUTPUT}

if [ $? -ne 0 ]; then
    cd
    OUTPUT2=magnus_results.log
    echo "$EXECUTABLE with jobID $SLURM_JOBID Failed" >> ${OUTPUT2}
else
    cd
    OUTPUT2=magnus_results.log
    echo "$EXECUTABLE with jobID $SLURM_JOBID Succeeded" >> ${OUTPUT2}
fi

cd $SCRATCH



\section{Conclusion}




\section{thebibliography}

\end{thebibliography}

\end{document}
% end document

% begin the document
\begin{Document}

\Title{Internship Report for Ranganai Mapondera and Ozlem Erkilic}
\Subtitle{Getexample: Reducing Barriers to Entry on Shared HPC Resources}

\author{\IEEEauthorblockN{Ranganai Mapondera}
\IEEEauthorblockA{Pawsey Supercomputing Centre\\
Curtin University\\
Perth, Australia\\
Email:ranganai.mapondera@student.curtin.edu.au }
\and
\IEEEauthorblockN{Ozlem Erkilic}
\IEEEauthorblockA{Pawsey Supercomputing Centre\\
Curtin University\\
Perth, Australia\\
Email:ozlem.erkilic@student.curtin.edu.au}
\\


\Section{Abstract}
\begin{abstract}

The best way to understand how to compute a task based on some resources, constraints and goals on advanced computing systems is to work through sample 
codes. A lot of expert users browse through the Internet to find these examples that they can slightly modify to achieve the results they expect. 
However, this can be very diffucult for some users who are new to this environment as there are always so many of these examples which are not 
always what they specifically need. This brings out the challenge of having to modify the example to suit their requirements on a particular computing 
resource, mainly the High Performance Computing systems (HPC) since the modified examples mostly do not work due to HPC systems having different set ups.
Therefore, it is hard to tell what the problem is for inexperienced users. Quite often, the users maynot be able to tell if the example is broken or if 
the example is correct, but maybe the code requires some adjustments. This is a very common problem encountered by the users of Pawsey Supercomputing 
Centre. For this reason, this paper provides solutions to how to run and submit examples on different resources such as supercomputers of Pawsey 
Supercomputing Centre through the use of a simple tool "getexample". The "getexample" is an effective tool that is developed for the supercomputers of 
Pawsey that are dependent on a command line interface and aims to help their users learn to how to run code mainly parallel programming examples, submit 
these examples to different operating systems of Pawsey such as Magnus, Zeus and Zythos. Although, the getexample is limited to these HPC resources, it 
has the potential to be expanded to other resources and interfaces. 


\end{abstract}
\\
\Section{Introduction}
\begin{Introduction}

Even though there are many sample codes on search engines for programming purposes, it is usually very rare that these codes work at the first try 
on different high performance computing systems without modifying them. This is mainly due to each advanced computing system being built up differently
from each other to perform specific tasks for the needs of the users. At Pawsey Supercomputing Centre, there are various HPC resources where each of them
are set up with slight variations from each other. Therefore, this becomes very challanging for the users, especially the novices to change the example 
codes to display a working task on Pawsey's resources as there are many differences between each resources with many constraints. The most common 
differences between their systems are as follow:

1) Various operating systems 
2) Different program environments
3) Different compiler options with varying commands
4) Different compiler flags and wrappers for varying compiler options
5) Different module systems and paths
6) Different module naming conventions
7) Different versions of libraries
8) Different personal scratch, group and home directories
9) Different scheduling policies

For this reason, when the user runs a sample code found from the Internet for each supercomputers (Magnus, Zeus and Zythos), the code fails to compile 
successfully and hence, does not run as each individual system has specific operating system and commands set up for them. To rectify these problems, 
some HPC resources provide their users websites with working examples that aim to assist them how to carry out their tasks step by step. 
However, sometimes the commands in these examples can be outdated due to updates in compilers and the operating systems. They also can be challenging 
to follow and perform on the real systems for the users who are not very experienced with these resources. Although, one may find a well-written bash 
script to run these systems, may not know how to make this script executable by using the chmod commands. Furthermore, a user may use a sample source 
code such as a basic MPI code which includes some mpi libabraries to perform a particular job on these supercomputers but may fail when compiled due 
to these libraries being no longer valid or upgraded to a different version. Or one may complete their task successfully, but may not know in which 
filesystem to store their results, for example in scratch or their group because some supercomputers have policies on how long to keep the data on 
certain filesystems such as scratch. If they are not moved from that specific filesystem within a certain amount of time, the results get deleted and 
thus, the user loses their work. This is a major problem for the users, especially researchers and scientist since it takes very long time to collect 
their data and vital for their researches. It is also very important that these are stored correctly within these sources.

In order to minimise these problems, the getexample was developed to suplly examples which do not require further editing or modification and works
when the executable is run. Thus, it aims to teach the users of Pawsey Supercomputing Centre how to run and submit tasks on the supercomputers without 
encountering issues. The rest of this paper explains how the getexample tool works.   


\end{Introduction}
\
\
\Section{Overview of getexample}

The main aim of the "getexample" is to provide easy access to the examples.It is expected to be founded on the following terms:

1) Easy to use for everyone including beginners
2) Practical examples which can be modified or updated by other users for their own work
3) The example should be clearly and completely detailed with steps to help users to fathom and apply it

\Section{Scope}
\begin{scope}

This shows the expected results from this Internship at Pawsey Supercomputing Center
\
\



\end{scope}
\
\Section{Creating Examples}

The examples included in the getexample library were created by internship students at Pawsey with their supervisor while the source codes within the
getexample were obtained from some wiki pages and websites and were acknowledged in the examples. Some of the examples were also created due to the
needs of research students for their internship projects at Pawsey. The examples obtained in the getexample are mainly for teaching the users how to work
with parallel programming including MPI, OpenMP and hybrid jobs such as OpenMP/MPI which utilise basic source codes similar to "Hello world" program and
submit these jobs to different supercomputers such as Magnus, Zeus and Zythos using different program environments and compiler modules. 

Each individual example on the getexample occupies a directory that is reserved for them and each example consists of three files listed below:
    
1) SLURM (Simple Linux Utility for Resource Management) : This allows users to submit batch jobs to the supercomputers, check on their status and cancel 
them if needed. It contains the necessary information about the name of the executable, the results directory, the name of the output file and the jobID. 
It also allows the users to edit how many nodes the code requires to run on the HPC systems, the duration of the task that it takes, which partition to
be used and their account name. The SLURM initially creates a scracth directory for the example to run in and the results are outputted to a log file. 
Then, it creates a group directory in which the results directory is located for that example. Once the output file is completed within the 
scratch, the output file is then moved to the results directory located in the group directory and the scratch directory then gets removed.

2) Source Code : This is usually a source code in c or Fortran and taken from wiki pages to run the example.

3) README : This file is an executable bash script which can be read and run by the users. It provides information about what the source code does,
how to compile the source code depending on the program environment such as Cray, Intel, GNU or PGI, what can be modified in the SLURM directives, and 
a set of instruction on how to submit the SLURM to the supercomputers including which specific commands to use for particular supercomputers.

For the examples in the getexample tool to be user friendly, helpful and efficient for working with HPC resources, there are many design considerations 
to be held as listed below:

1) Before launching the examples to the users, it should be made sure that the examples run successfully without encountering any errors. For example,
they should be suitable for the current operating systems with the use of correct commands for the different compiler modules such as Intel, GNU, Cray
and PGI. 

2) All the files to run the example successfully should be included with the example.

3) To minimise confusion on how to perform the example on the supercomputers, the example should be executed with a single command. For example, 
if a simple shell script is used, it should be run as ./README.

4) The examples should have enough instructions about what each command does and which parts of the SLURM and the README file can be modified so that
all the files and the scripts should be able to edited easily by the users for their preferences.

5) It must be ensured that the README and SLURM have enough information about how to change certain parts of these files so that the users can run the 
example how they wish to on the supercomputers. These instructions should be understandable by the new users who are not very experienced with these
systems. For example, if the user wishes to use more nodes on the supercomputers, one should be able to know where to change it from.
 
Even though, the examples in the getexample tool are designed in a such way that once they are downloaded, the user should be able to run them without
having to modify them. However, this is not always the case as some of the supercomputers in Pawsey Supercomputing Centre such as Zythos due to
different set ups require the account name which is customized for each user and without the correct account name in the SLURM, the code fails to run. 
Therefore, the examples in the getexample tool ensures that the users are informed on how to change their account name located in the SLURM file.
Furthermore, they assist the users on how to change number of nodes used within the supercomputers and increase or reduce the number of cores used.     


\Subsection{Creating Examples on Magnus}

This is described as a Cray X40 supercomputer that consists of many nodes that are connected by a high speed network. The cray supercomputer consists of intel Xeon E5-2690 v3(Haswell) as its only processor across all compute nodes. For the compute nodes, each one of them has 2 processors and each of these 2 processors has 12 cores.
Magnus is specified to have 24 cores per node and in total these sum up to 35,712 cores across the 1488 nodes. Each of these nodes has a memory of 64GB. The maximum login nodes of magnus are 2.Each of these two nodes has 64GB of DDR4 memory that is shared between 24 cores.Of these 24 cores, each one of them has 32KB instruction and data caches, and one 256KB L2 cache; 12 of the cores share one 30 MB L3 cache.The memory of the system is 93TB in total.During this summer internship, the focus was to provide simplified getexamples which we developed and each othe directories described below consists of mainly the source code, the slurm file and the Readme . The source code details the script with the example code taken from online sources or developed to be the job script. The jobscript produces the executable file after being compiled. The slurm file is where a batch script is submitted. It runs the executable from the jobscript. The executable file will be used by the compiler. In the slurm, we have slurm directives where we choose the number of nodes or ntasks. We also specify the compiler before the executable. The Readme file is where the executable file is run. The slurm is given because it contains the compiler. The results from running the executable are submitted to Scratch which deletes the files after some time. A new group has been created which stores all the files from scratch and in the script the results are deleted from scratch and only viewed in group. Compute nodes are attached to scratch and are capable of moving back and forth.$
$Fortan and C code were used for most of the examples to illustrate the differences. Some wrappers were implemented as well for these compilers which are ftn,mpif90 and cc. When using these, it is important to invoke the actual programming environment being used PrgEnv-cray, PrgEnv-gnu or PrgEnv-intel.The relevant paths are organized via the module system.



\Subsubsection{MPI Examples}

For MPI examples, we used fortran and c compilers. We ran the jobs on cray, gnu and intel.
It is necessary to realise that we need to load each module we intend to work on by doing a a module load PrgEnv-(name of environment). 
If another module had been added before, it is best to swap or unload the currently loaded environment module and load the necessary module.
After loading the module, do a module list to see if the module has been added preferrably the latest version.
To do MPI jobs on a cray,determine whether it is for a C or fortran code for the source code.
A lot of MPI jobs are said to be more effective with 24MPI tasks per node with one MPI task running on each core.

The SLURM script shows the choice in number of nodes, which are 2 for MPI tasks. 
The wall-clock time limit varies but for two fully occupied MPI tasks, we chose one minute.
Note that for Magnus, we express the partition as debugq as one of the slurm directives.The export=NONE should be included and this is all expressed as:

#SBATCH --partition=debugq
#SBATCH --nodes=2
#SBATCH --time=00:01:00
#SBATCH --export=NONE

We make use of aprun to launch MPI jobs, in this case we have to choose 48 MPI tasks with 24 MPI tasks per node.
To express this in slurm we wrote as:

aprun -n 48 -N 24 ./code_mpi.x

We additionally have to avoid any inadvertent OpenMP threading by setting OMP_NUM_THREADS to 1:

export OMP_NUM_THREADS=1

The aprun changes when we have to use 2 partially occupied nodes , in this case there is a distribution of the 24MPI tasks to 2 nodes, each supplying 64GB and each running 12 MPI tasks.

aprun -n 24 -N 12 -S 6 ./code_mpi.x

As shown in the aprun, for 2 partially occupied nodes we also put 6 tasks per socket.
Everything else does not change from the fully occupied nodes except of the aprun to launch the job.
The name of the executable is also given in the SLURM script and it is shown where this executable will be run.
It is vital to give the name of the output file by using commands:

OUTPUT=mpifortran_cray.log
 
For this report, the job is run in scratch , a directory that deletes, so the job is sent to Group, a long-lasting directory. 
The job is run and results are viewed in GROUP when we do a cat of the OUTPUT file. 
The results are only viewed  after an immediate deletion of the SCRATCH directory.

In the bash script for MPI i.e README, we write the code to compile which is 

ftn -O2 hello_mpi.f90 -o hello_mpi_cray if we are using fortran source code which is in this case hello_mpi.f90. 
To direct to the executable file,a -o is placed after the source code and the executable comes immetiadely after it.
To submit the job to magnus, do an sbatch and write the slurm file in the bash script.

sbatch hello_mpi_cray.slurm

If the source code in use is C code, the SLURM is not different from that of Fortran. However, the README does change. 
The code to compile changes and is written as:

cc -O2 hello_mpi.c -o hello_mpi_cray

The rest of the bash script remains the same way it was with fortran and the results are located and directed the same way.
When using gnu or intel as the module environment, we have to swap with the previously loaded module and also do a module list. 

module unload PrgEnv-cray
module load PrgEnv-gnu

then for intel

module unload PrgEnv-gnu
module load PrgEnv-intel

It is easier when it is included in the SLURM file although regardless of that, it should be done manually.
The bash script has no changes needed to be made to it as long as the right module is loaded.


\Subsubsection{OMP Examples}

OpenMP may be thought of one task because of their restrictions to only one node.
The node can be fully occupied node or a node occupying a single NUMA region.
When it is fully occupied, the slurm script has a choice of only one node as explained.

#SBATCH --partition=debugq
#SBATCH --nodes=1
#SBATCH --time=00:01:00
#SBATCH --export=NONE

The launching of the job by aprun is done by specifying that there is one task and 24 threads. 
The number of threads is also called a "depth" of 24.
The OMP_NUM_THREADS should also be set to 24 giving an expression of:

export OMP_NUM_THREADS=24
aprun -n 1 -d 24 ./code_omp.x

For a node occupying a single NUMA region, the code is more efficient when threads are limited to one NUMA region containing 12 cores. 
The expression -d 12 ensures that threads are bound correctly but this can be clearly done by specifying this via cc.
This is all illustrated below as:

export OMP_NUM_THREADS=12
aprun -n 1 -d 12 -cc 0-11 ./code_omp.x

If the source code being run is fortran,on cray the README shows the compiling code as:

ftn -O2 -h omp omp_hello.f -o hello_omp_cray for fortran and
cc -O2 -h omp omp_hello.c -o hello_omp_cray for C code

The job is also submitted to Magnus by using the sbatch before the slurm as previously mentioned.
It is important to remember the module before running the job, to load it first accordingly.

When using gnu, the compiler changes to 
ftn -O2 -fopenmp omp_hello.f -o omp_hello_gnu
cc -O2 -fopenmp omp_hello.c -o omp_hello_gnu
 
For fortran and C codes respectively.

For intel, the compiler used is expressed as:
ftn -O2 -openmp omp_hello.f -o omp_hello_intel
cc -O2 -openmp omp_hello.c -o omp_hello_intel

\Subsubsection(Hybrid Examples}

A hybrid job is a mixed job between OpenMP and MPI. It aims to take advantage of the OpenMP in NUMA region which is also known as a socket.
The OpenMP is in this case involved with one 12 core chip and runs on 6 threads on each of 8 MPI tasks which dispersed evenly between the NUMA regions.
A sum of 2 nodes is required which accomodates 8MPI tasks and 6 threads.
Besides specifying the number of nodes, the time is also shown in the slurm script as done in the previous examples.In this case we could choose 5 minutes as done in the getexamples.
The partition remains the same debugq.

#SBATCH --partition=debugq
#SBATCH --nodes=2
#SBATCH --time=00:05:00
#SBATCH --export=NONE

8MPI tasks(n) are specified to aprun to launch the job.This is specified also with 4 MPI tasks per node(N) per socket (-S 2).
Each of the 8 MPI tasks will have 6 threads (-d 6). The OMP_NUM_THREADS is also expressed as 6.
The full expression is shown as:

export OMP_NUM_THREADS=6
aprun -n 8 -N 4 -S 2 -d 6 ./code_hybrid.x

The bash script will show the compiling code as:
ftn -O2 -h omp hybrid_hello.f90 -o hello_hybrid_cray
cc -O2 -h omp hybrid_hello.c -o hello_hybrid_cray

For each of the source codes, fortran and C.

Again in this example, cray module should be loaded as:
module load PrgEnv-cray or module swap with the previously loaded module environment.

For gnu environment,for fortran and C codes respectively, the README shows the compiler code as:

ftn -O2 -fopenmp hybrid_hello.f90 -o hello_hybrid_gnu
cc -O2 -fopenmp hybrid_hello.c -o hello_hybrid_gnu

The job is submitted to Magnus by sbatch{slurm script}

The environment is then changed first before running the job:

module swap PrgEnv-cray PrgEnv-gnu

or unload cray first
module unload PrgEnv-cray
module load PrgEnv-gnu
module list

The instructions to run the executable are given in the README  file as:

./README 
This applies for all the getexamples.

In intel the compiler code changes as well to:

ftn -O2 -openmp hybrid_hello.f90 -o hello_hybrid_intel
cc -O2 -openmp hybrid_hello.c -o hello_hybrid_intel


\Subsection{Zeus}

The getexample tool also provides examples for Zeus which is according to Pawsey system descriptions an SGI Linux cluster that is principally utilized 
for pre and post-preparing of data, extensive shared memory calculations and remote visualization work. It is diverse group with 30 nodes in different layouts. It shares the same /home,/scratch and 
/group file systems with Magnus. To access all the 30 nodes, the client has to ssh to zeus.pawsey.org.au.
Zeus contains one large-memory nodes known as Zythos, which is SGI UV2000 system. Zythos contains 24 UV blades which show up as a solitary framework 
with large amounts of shared memory of 6TB.Out of 24 blades, 20 of them contain two 6-core Intel Xeon E5-4610 CPUs and the remnant contain only one of 
these CPUs and one other Nvidia Tesla K20 GPU. Numerous clients are allowed to keep running on Zythos simultaneously. Aside from Zythos, the other 29 
nodes of Zeus all have two 8-core Intel Xeon E5-2670 CPUs. GPUs are also featured in 27 of these with the 20 containing each one of them Nvidia Tesla 
K20 GPU. Only one user is allowed to work on each of the nodes at a time. Zeus is part of the arrangement of Pawsey Project assets and it is located at 
the Pawsey Supercomputing Center and it is incorporated with the rest of the infrastructure which permits a different scope of work.
 


\Subsubsection{MPI Jobs}

The getexample tool also includes MPI examples for Zeus with GNU, Intel and PGI compilers because both Zeus and Magnus have different operating systems 
and hence, the same MPI example needs to handled differently on Zeus. It is important to notice that the MPI source codes used for Zeus are the same
as Magnus, but the commands to run the same source codes are different on Zeus as compared to Magnus. Additionally, when the same code is run 
with different compilers such as GNU, Intel and PGI, the compiler module for the required compiler must be loaded as Magnus. This information was
included in the README and SLURM files of each example within the getexample tool and the required module for the specific compiler was already loaded 
within these files so that the user is not asked to download them manually to prevent any mistakes while running the code.

The two common source codes used for the MPI examples were hello_mpi.f90 (a FORTRAN 90 source code) and hello_mpi.c (a c code). These codes were used 
with GNU, Intel and PGI compiler options on Zeus to assist the users on how to run the same code on Zeus with different modules, even if the command 
for compiling the same source code does not for MPI tasks on Zeus.

As mentioned earlier, both codes were run on both Magnus and Zeus on 2 nodes. However, the partition in the SLURM was changed to workq from debugq to 
workq and to run the JOB on Zeus as shown:

#SBATCH --partition=workq
#SBATCH --nodes=2

Unlike Magnus, when --export=NONE is included in the SLURM for Zeus, the code does not compile and gives many errors. For this reason, this line was
excluded in the SLURM files of Zeus to compile the codes correctly.

To run the code with a given executable on scratch, and store the results in a directory within the GROUP directory to an output file, the generic 
variables were created in the SLURM as shown:

EXECUTABLE=hello_mpi_gnu
SCRATCH=$MYSCRATCH/run_hostname/$SLURM_JOBID
RESULTS=$MYGROUP/hellompi_gnu_results_zeus/$SLURM_JOBID

OUTPUT=hello_mpi_gnu.log

On Magnus, a total of 48 MPI tasks could be run whereas this was too much for Zeus and thus, the number of MPI tasks was reduced to 32. To run the job
on Zeus, instead of using aprun command used in Magnus, srun command was used as srun is specific to Zeus while aprun is designed for Magnus. Therefore,
to run the code, the following command was used as:

srun -n 32 -N 2 --mpi=pmi2 ./$EXECUTABLE >> ${OUTPUT}

where -n defines the total number of MPI tasks while -N defines the number of used on Zeus which is different from Magnus as -N defines the number of
MPI tasks per node. Once again, in comparison to Magnus --mpi=pmi2 was added to srun which wasn't used in aprun where --mpi=pmi2 is the MPI
implementation and must specified to srun for the correct operation.

In the README file, depending on which compiler the source code would be compiled with, the compiler module was included and loaded in the README file 
when it ran executed. For example, if the GNU compiler module was to be used, the gcc module was loaded with the mpt module as shown below:

module load gcc
module load mpt

If the code was to be compiled with Intel, the gcc module needs to unloaded and the Intel compiler module needs to be loaded. This applies to the PGI
compiler as well, after the gcc module is unloaded, the pgi module should be loaded as shown below:

For Intel:
module unload gcc
module load intel
module load mpt

For PGI:
module unload gcc
module load pgi
module load mpt

The mpt module needs to be loaded whenever an MPI or OpenMP/MPI task is submitted to Zeus as it provides the SGI message to access the MPI library.

Additionally, the README file contains the command to compile the FORTRAN 90, hello_mpi.f90 code  and hello_mpi.c c code with the GNU or Intel compiler 
as shown:

mpif90 hello_mpi.f90 -o hello_mpi_gnu
mpicc hello_mpi.c -o hello_mpi_gnu

For MPI tasks as mentioned earlier, the command to compile the code does not change for GNU and Intel compilers, whereas it differs for PGI compiler.
Hence to compile the basic hello_mpi.f90 and hello_mpi.c code, the following commands were used:

pgf90 -Mmpi=sgimpi hello_mpi.f90 -o hello_mpi_pgi
pgcc -Mmpi=sgimpi hello_mpi.c -o hello_mpi_c 
  

\Subsubsection{OMP Examples}

The OpenMP examples used for the Zeus are the same source codes as the ones used for Magnus which are omp_hello.c (c code) and omp_hello.f (FORTRAN).
However, as mentioned earlier due to the different setups between Magnus and Zeus, the SLURM and the README for these codes are different.

The main difference between the MPI and OpenMP jobs for Zeus is that the compiling command changes for all the different compiler options with OpenMP
commands as the wrappers are not the same for GNU, Intel and PGI compilers. It is vital to notice that the commands to compile hello_mpi.f90 and
hello_mpi.c were the same for GNU and Intel compilers for MPI jobs, whereas this is not the same case for OpenMP jobs anymore. However, everytime the
compiler is swapped from one to the other, the correct compiler module should be loaded because the default compiler module is gcc. If not, the system will fail to recognize the compiler commands
and will end up giving errors, even if the source codes and the SLURM files work.

In the getexample tool, to run an OpenMP job on Zeus only one node was used just as Magnus, but as diccussed in the MPI example, the workq partition was
used unlike the  debugq partition on Magnus. Therefore, both of the OpenMP examples run one 16-thread OpenMP instance with one node.

To run the omp_hello.f and omp_hello.c code on Zeus with GNU compiler, the number of OpenMP threads were set to 16 on the SLURM and srun command was 
used to run it as shown:

export OMP_NUM_THREADS=16
srun -n 1 -c $OMP_NUM_THREADS ./$EXECUTABLE >> ${OUTPUT}

An alternative way of running the same job could be using the omplace command.

omplace -nt $OMP_NUM_THREADS -tm open64 ./$EXECUTABLE >> ${OUTPUT}

The srun command above can be used for both Intel and PGI compilers without requiring modification. However, the omplace command would be different for
the other compilers. It is important to note that -tm open64 should be included above in the omplace command because when compiling with GNU, this 
command will not be identified as the default thread model is intel. Hence, -tm open64 tells that the compiler module is GNU.

Therefore, to run this job with the Intel compiler, the omplace command in the SLURM would look like:

omplace -nt $OMP_NUM_THREADS ./$EXECUTABLE >> ${OUTPUT}

To compile the omp_hello.f with the GNU compiler, the following command was used in the README file:

gfortran -O2 -fopenmp omp_hello.f -o hello_omp_gnu

The omp_hello.c code can be compiled with the GNU as shown below:

gcc -O2 -fopenmp omp_hello.c -o hello_omp_gnu

As it can be seen, the only difference between compiling the c code and the FORTRAN code is the gcc and gfortran. Both of these codes can be compiled
with Intel and PGI compilers, but the wrapper for them will different.

For Intel:
ifort -O2 -qopenmp omp_hello.f -o hello_omp_intel
icc -O2 -qopenmp omp_hello.c -o hello_omp_intel

For PGI:
pgfortran -O2 -mp omp_hello.f -o hello_omp_pgi
pgcc -mp omp_hello.c -o hello_omp_pgi


\Subsubsection{Hybrid Examples}

The OpenMP/MPI hybrid codes used for Zeus were exactly same as the ones for Magnus such as hybrid_hello.f90 and hello_hybrid.c, but they both had
distinct compiler commands specific to each compiler module. Therefore, it was also necessary to include which module to be loaded in the README files
of the getexample tool.

This hybrid job requires 2 nodes and runs 1 MPI process with 16 OpenMP threads on each compiled executable. In order to launch the job to Zeus for both 
of the source codes, the number of OpenMP threads was set to 16 and the srun command was used.

export OMP_NUM_THREADS=16
srun --mpi=pmi2 -n 2 -N 2 ./$EXECUTABLE >> ${OUTPUT}

This command was used for all of the compilers without making any changes. To compile the hybrid_hello.f90 and hello.c, the following commands were used:

For GNU:
mpif90 -O2 -fopenmp hybrid_hello.f90 -o hello_hybrid_gnu
mpicc -fopenmp -O2 hello_hybrid.c -o hello_hybrid_gnu

For Intel:
mpif90 -O2 -qopenmp hybrid_hello.f90 -o hello_hybrid_intel
mpicc -qopenmp hello_hybrid.c -o hello_hybrid_intel

For PGI:
pgf90 -Mmpi=sgimpi -mp hybrid_hello.f90 -o hello_hybrid_pgi
pgcc -Mmpi=sgimpi -mp hello_hybrid.c -o hello_hybrid_pgi

\Subsubsection{CUDA Examples}

The CUDA programming is a heterogeneous model where both CPU and GPU nodes are used. In CUDA, the host refers to the CPU and its memory, whereas the 
device refers to the GPU an its memory. Therefore, a CUDA code running on the host have access to the memory on the host as well as the device. It also 
executes kernel functions on the device which are executed by GPU threads in parallel. A basic CUDA works by declaring and allocating host and device 
memory. Then, it initializes the host data and transfers the data from the host to the device. Once it executes the kernel function, it transfers the 
results from the device to the host.

The getexample tool includes a basic Hello world CUDA code for Zeus. It uses 

\Subsection{Zythos}

Being one of the large memory nodes of Zeus, this framework is said to be an SGI UV2000 which gives access of up to 6TB of shared memory in cache-coherent non-uniform memory access (ccNUMA) mold.
The hidden equipment depends on Intel Xeon E5-4610 2.4 GHz innovation which gives 6-core nodes(*). 
Every core has about 32KB L1 direction and information reserves and one 256KB L2 memory store. 
Each of the 6-core nodes shares a 15MB L3 cache,and each node gives 128 GB main memory: 256GB per two-second, or two-node, blade.

In the system, four blades contain one Intel Xeon 6-core chip plus one Tesla K20 GPU card.
Worldwide store coherency is guaranteed by method of unique equipment which gets to physically remote memory over the system with low inertness and high data transfer capacity.
Zythos is a restricted asset and is liable to strict qualification criteria. 
One of the following criteria for every task must be met:

1) A vast informational collection that must be held in shared memory,and more networthy than 512GB.
2) Gigantic thread-level parallelism, for example,utilizing tens or many CPU cores.

It is more preferrable that the work meets both criteria. 
On the off chance that the work does not satisfy both criteria, then Zeus is a more fitting asset.
The extensive most extreme walltime on Zythos is necessary to help investigate these expansive informational collections.
Having programming with long runtimes or tak-level parallelism does not justify access to Zythos.
As such most of the work has to be changed the group name in order to run it. 
Specific accounts are allowed to run on Zythos. It is necessary to change account name and put the authorized account otherwise your job will not run.

Zythos has no direct access, all work is facillitated via Zeus.Login to Zeus the usual way then access Zythos in Zeus.
The advancement environment for Zythos is the as Zeus, this includes compilers, libraries,etc. 
It is therefore beneficial Zeus first before trying to work on Zythos.
Zeus is also used to submit the jobs for Zythos but the partition in SLURM changes to zythos and shown as:

#SBATCH --partition=zythos

The utilization of omplace for thread placement is identical to Zeus and detailed in the previous section.
Jobs that require message passing must make use of mpirun job launcher which is given by the SGI message passing toolkit done by making use of the shared memory mechanisms in equipment accessible on Zythos.
For better memory execution, clients are recommended to ask for assets on Zythos as entire cores, or entire CPU sets instead of letting SLURM desgnate accessible cpus sporadically on the machine. 
The basic approach to do this is by keeping -cpus-per-task at 6 at all times and conform -ntasks to suit the cpu/memory prerequisites of the job.
In so doing, SLURM will be able to allocate cpu cores successively, rather than having to pick up available cpu cores which may be scattered all over the framework.
The primary motivation behind thid is to enhance data locality and memory perfomance.

The jobs that are run on Zythos can be run on any of the three environments gnu, intel and pgi instead of cray as on Magnus.



\Subsubsection{MPI Examples}

As previously done, examples are utilizing code from either c or fotran source code.
The SLURM for MPI jobs shows the request for 24 cores by asking for 4 ntasks and 6 -cpus-per-task and nodes is not. 
This means the job has access to a total of 512GB.
The final SLURM directives are detailed as:

#SBATCH --partition=zythos
#SBATCH --natasks=4
#SBATCH --cpus-per-task=6
#SBATCH --account=pawsey0001
#SBATCH --time=00:10:00

Note that the account has to be changed to an account authorised to access Zythos.
As done on all examples, time has been specified.
The default loaded module is gcc which is for running on GNU.
To load the latest version of GNU do a module load.It is essential in this case to load mpt module otherwise the job will not run.

module load gcc
module load mpt
module list

Create an executable as done in all other examples and redirect results to MYGROUP after SCRATCH
Message passing in done by mpirun and the executable is redirected to the OUTPUT and the resulting command is written as:

mpirun -np 24 ./$EXECUTABLE >> ${OUTPUT}

The removal of SCRATCH directory is also done in SLURM file as in the default.
README or bash script is run by ./README.
It contains the command to compile the hello_mpi.f90 or hello_mpi.c code as:

mpif90 -O2 hello_mpi.f90 -o hello_mpi_gnu for fortran code or
mpicc -O2 hello_mpi.c -o hello_mpi_gnu for c code

To submit the job to Zythos:

sbatch hello_mpi_gnu.slurm

Working on Intel would require swapping of modules from gcc to intel by manually using commands:

module swap gcc intel
module load mpt
module list

All the commands on the SLURM and README remain the same because of the use of wrappers: mpif90 and mpicc for fortran and c code respectively.

For pgi environment, the READE compiles with a different code shown as:

pgf90 -Mmpi=sgimpi hello_mpi.f90 -o hello_mpi_pgi for fortran code and 
pgcc -Mmpi=sgimpi hello_mpi.c -o hello_mpi_pgi for c code

Submit the job to Zythos :
sbatch hello_mpi_pgi.slurm

The SLURM does not change much except of the names of OUTPUT and results directory.
The message is passed through mpirun for all examples on Zythos.
Before running the job, load the pgi module and other necessary modules by swapping from intel.

module swap intel pgi
module load mpt
module list

Run the job ./README and use jobID to view results in $MYGROUP directory.





\Subsubsection{OMP Examples}

The SLURM script for OpeMP requests for 2 nodes and 6 cores which is a total of 12 cores. 
The total memory carried is about 128GB in each of them.
We specify the 2 nodes as ntasks rather than nodes in the SLURM directives.

#SBATCH --partition=zythos
#SBATCH --ntasks=2
#SBATCH --cpus-per-task=6
#SBATCH --account=pawsey0001
#SBATCH --time=00:10:00

As usual, the OUTPUT name should be declared as a log file
OUTPUT=hello_omp_gnu.log

Omplace is used in OpenMP to control thread placement with a default of about 6 threads per node. 
This gives a total of 12 threads in this case since 2 nodes are being utilized.
The commands as written in the SLURM  is:

export OMP_NUM_THREADS=12
omplace -nt $OMP_NUM_THREADS ./$EXECUTABLE >> ${OUTPUT}

To compile the omp_hello.f90 for fortran with GNU we use:
gfortran -O2 -fopenmp omp_hello.f90 -o hello_omp_gnu

To compile the omp_hello.c code with GNU we use:
gcc -O2 -fopen omp_hello.c -o hello_omp_gnu

and to submit the job to Zythos
sbatch hello_omp_gnu.slurm

Although gcc is the default, it is best to load the module and other necessary modules manually and use the latest version of gcc.

module load gcc
module load mpt
module list
 
Intel compiler is different from GNU and for fortran it is written as:

ifort -O2 -qopenmp omp_hello.f90 -o hello_omp_intel

The c code compiler in the bash script is written as:

mpicc -O2 hello_mpi.c -o hello_mpi_intel

The sbatch{SLURM} is done to submit the job to Zythos




\Subsubsection{Cuda Examples}



\Subsubsection{Hybrid Examples}


\Section{Other Completed Tasks}

\Subsection{Intel Xeon Processor}

\Subsection{MIC &MKL}


\Susection{Ubuntu}



\Subsection{Supercomputing Examples to Others}



\Section{Conclusion}




\Section{References}
\end{Document}
% end document

% begin the document
\begin{Document}

\Title{Internship Report for Ranganai Mapondera and Ozlem Erkilic}
\Subtitle{Getexample: Reducing Barriers to Entry on Shared HPC Resources}

\author{\IEEEauthorblockN{Ranganai Mapondera}
\IEEEauthorblockA{Pawsey Supercomputing Centre\\
Curtin University\\
Perth, Australia\\
Email:ranganai.mapondera@student.curtin.edu.au }
\and
\IEEEauthorblockN{Ozlem Erkilic}
\IEEEauthorblockA{Pawsey Supercomputing Centre\\
Curtin University\\
Perth, Australia\\
Email:ozlem.erkilic@student.curtin.edu.au}
\\


\Section{Abstract}
\begin{abstract}

The best way to understand how to compute a task based on some resources, constraints and goals on advanced computing systems is to work through sample 
codes. A lot of expert users browse through the Internet to find these examples that they can slightly modify to achieve the results they expect. 
However, this can be very diffucult for some users who are new to this environment as there are always so many of these examples which are not 
always what they specifically need. This brings out the challenge of having to modify the example to suit their requirements on a particular computing 
resource, mainly the High Performance Computing systems (HPC) since the modified examples mostly do not work due to HPC systems having different set ups.
Therefore, it is hard to tell what the problem is for inexperienced users. Quite often, the users maynot be able to tell if the example is broken or if 
the example is correct, but maybe the code requires some adjustments. This is a very common problem encountered by the users of Pawsey Supercomputing 
Centre. For this reason, this paper provides solutions to how to run and submit examples on different resources such as supercomputers of Pawsey 
Supercomputing Centre through the use of a simple tool "getexample". The "getexample" is an effective tool that is developed for the supercomputers of 
Pawsey that are dependent on a command line interface and aims to help their users learn to how to run code mainly parallel programming examples, submit 
these examples to different operating systems of Pawsey such as Magnus, Zeus and Zythos. Although, the getexample is limited to these HPC resources, it 
has the potential to be expanded to other resources and interfaces. 


\end{abstract}
\\
\Section{Introduction}
\begin{Introduction}

Even though there are many sample codes on search engines for programming purposes, it is usually very rare that these codes work at the first try 
on different high performance computing systems without modifying them. This is mainly due to each advanced computing system being built up differently
from each other to perform specific tasks for the needs of the users. At Pawsey Supercomputing Centre, there are various HPC resources where each of them
are set up with slight variations from each other. Therefore, this becomes very challanging for the users, especially the novices to change the example 
codes to display a working task on Pawsey's resources as there are many differences between each resources with many constraints. The most common 
differences between their systems are as follow:

1) Various operating systems 
2) Different program environments
3) Different compiler options with varying commands
4) Different compiler flags and wrappers for varying compiler options
5) Different module systems and paths
6) Different module naming conventions
7) Different versions of libraries
8) Different personal scratch, group and home directories
9) Different scheduling policies

For this reason, when the user runs a sample code found from the Internet for each supercomputers (Magnus, Zeus and Zythos), the code fails to compile 
successfully and hence, does not run as each individual system has specific operating system and commands set up for them. To rectify these problems, 
some HPC resources provide their users websites with working examples that aim to assist them how to carry out their tasks step by step. 
However, sometimes the commands in these examples can be outdated due to updates in compilers and the operating systems. They also can be challenging 
to follow and perform on the real systems for the users who are not very experienced with these resources. Although, one may find a well-written bash 
script to run these systems, may not know how to make this script executable by using the chmod commands. Furthermore, a user may use a sample source 
code such as a basic MPI code which includes some mpi libabraries to perform a particular job on these supercomputers but may fail when compiled due 
to these libraries being no longer valid or upgraded to a different version. Or one may complete their task successfully, but may not know in which 
filesystem to store their results, for example in scratch or their group because some supercomputers have policies on how long to keep the data on 
certain filesystems such as scratch. If they are not moved from that specific filesystem within a certain amount of time, the results get deleted and 
thus, the user loses their work. This is a major problem for the users, especially researchers and scientist since it takes very long time to collect 
their data and vital for their researches. It is also very important that these are stored correctly within these sources.

In order to minimise these problems, the getexample was developed to suplly examples which do not require further editing or modification and works
when the executable is run. Thus, it aims to teach the users of Pawsey Supercomputing Centre how to run and submit tasks on the supercomputers without 
encountering issues. The rest of this paper explains how the getexample tool works.   


\end{Introduction}
\
\
\Section{Overview of getexample}

The main aim of the "getexample" is to provide easy access to the examples.It is expected to be founded on the following terms:

1) Easy to use for everyone including beginners
2) Practical examples which can be modified or updated by other users for their own work
3) The example should be clearly and completely detailed with steps to help users to fathom and apply it

\Section{Scope}
\begin{scope}

This shows the expected results from this Internship at Pawsey Supercomputing Center
\
\



\end{scope}
\
\Section{Creating Examples}

The examples included in the getexample library were created by internship students at Pawsey with their supervisor while the source codes within the
getexample were obtained from some wiki pages and websites and were acknowledged in the examples. Some of the examples were also created due to the
needs of research students for their internship projects at Pawsey. The examples obtained in the getexample are mainly for teaching the users how to work
with parallel programming including MPI, OpenMP and hybrid jobs such as OpenMP/MPI which utilise basic source codes similar to "Hello world" program and
submit these jobs to different supercomputers such as Magnus, Zeus and Zythos using different program environments and compiler modules. 

Each individual example on the getexample occupies a directory that is reserved for them and each example consists of three files listed below:
    
1) SLURM (Simple Linux Utility for Resource Management) : This allows users to submit batch jobs to the supercomputers, check on their status and cancel 
them if needed. It contains the necessary information about the name of the executable, the results directory, the name of the output file and the jobID. 
It also allows the users to edit how many nodes the code requires to run on the HPC systems, the duration of the task that it takes, which partition to
be used and their account name. The SLURM initially creates a scracth directory for the example to run in and the results are outputted to a log file. 
Then, it creates a group directory in which the results directory is located for that example. Once the output file is completed within the 
scratch, the output file is then moved to the results directory located in the group directory and the scratch directory then gets removed.

2) Source Code : This is usually a source code in c or Fortran and taken from wiki pages to run the example.

3) README : This file is an executable bash script which can be read and run by the users. It provides information about what the source code does,
how to compile the source code depending on the program environment such as Cray, Intel, GNU or PGI, what can be modified in the SLURM directives, and 
a set of instruction on how to submit the SLURM to the supercomputers including which specific commands to use for particular supercomputers.

For the examples in the getexample tool to be user friendly, helpful and efficient for working with HPC resources, there are many design considerations 
to be held as listed below:

1) Before launching the examples to the users, it should be made sure that the examples run successfully without encountering any errors. For example,
they should be suitable for the current operating systems with the use of correct commands for the different compiler modules such as Intel, GNU, Cray
and PGI. 

2) All the files to run the example successfully should be included with the example.

3) To minimise confusion on how to perform the example on the supercomputers, the example should be executed with a single command. For example, 
if a simple shell script is used, it should be run as ./README.

4) The examples should have enough instructions about what each command does and which parts of the SLURM and the README file can be modified so that
all the files and the scripts should be able to edited easily by the users for their preferences.

5) It must be ensured that the README and SLURM have enough information about how to change certain parts of these files so that the users can run the 
example how they wish to on the supercomputers. These instructions should be understandable by the new users who are not very experienced with these
systems. For example, if the user wishes to use more nodes on the supercomputers, one should be able to know where to change it from.
 
Even though, the examples in the getexample tool are designed in a such way that once they are downloaded, the user should be able to run them without
having to modify them. However, this is not always the case as some of the supercomputers in Pawsey Supercomputing Centre such as Zythos due to
different set ups require the account name which is customized for each user and without the correct account name in the SLURM, the code fails to run. 
Therefore, the examples in the getexample tool ensures that the users are informed on how to change their account name located in the SLURM file.
Furthermore, they assist the users on how to change number of nodes used within the supercomputers and increase or reduce the number of cores used.     


\Subsection{Creating Examples on Magnus}

This is described as a Cray X40 supercomputer that consists of many nodes that are connected by a high speed network. The cray supercomputer consists of intel Xeon E5-2690 v3(Haswell) as its only processor across all compute nodes. For the compute nodes, each one of them has 2 processors and each of these 2 processors has 12 cores.
Magnus is specified to have 24 cores per node and in total these sum up to 35,712 cores across the 1488 nodes. Each of these nodes has a memory of 64GB. The maximum login nodes of magnus are 2.Each of these two nodes has 64GB of DDR4 memory that is shared between 24 cores.Of these 24 cores, each one of them has 32KB instruction and data caches, and one 256KB L2 cache; 12 of the cores share one 30 MB L3 cache.The memory of the system is 93TB in total.During this summer internship, the focus was to provide simplified getexamples which we developed and each othe directories described below consists of mainly the source code, the slurm file and the Readme . The source code details the script with the example code taken from online sources or developed to be the job script. The jobscript produces the executable file after being compiled. The slurm file is where a batch script is submitted. It runs the executable from the jobscript. The executable file will be used by the compiler. In the slurm, we have slurm directives where we choose the number of nodes or ntasks. We also specify the compiler before the executable. The Readme file is where the executable file is run. The slurm is given because it contains the compiler. The results from running the executable are submitted to Scratch which deletes the files after some time. A new group has been created which stores all the files from scratch and in the script the results are deleted from scratch and only viewed in group. Compute nodes are attached to scratch and are capable of moving back and forth.$
$Fortan and C code were used for most of the examples to illustrate the differences. Some wrappers were implemented as well for these compilers which are ftn,mpif90 and cc. When using these, it is important to invoke the actual programming environment being used PrgEnv-cray, PrgEnv-gnu or PrgEnv-intel.The relevant paths are organized via the module system.



\Subsubsection{MPI Examples}

For MPI , we used fortran and c compilers. We ran the jobs on cray, gnu and intel.
It is necessary to realise that we need to load each module we intend to work on by doing a a module load PrgEnv-(name of environment). 
If another module had been added before, it is best to swap or unload the currently loaded environment module and load the necessary module.
After loading the module, do a module list to see if the module has been added preferrably the latest version.
To do MPI jobs on a cray,determine whether it is for a C or fortran code for the source code.
A lot of MPI jobs are said to be more effective with 24MPI tasks per node with one MPI task running on each core.

The slurm script shows the choice in number of nodes, which are 2 for MPI tasks. The wall-clock time limit varies but for two fully occupied MPI tasks, we chose one minute and this is expressed as:

#SBATCH --nodes=2
#SBATCH --time=00:01:00
#SBATCH --export=NONE

We make use of aprun to launch MPI jobs, in this case we have to choose 48 MPI tasks with 24 MPI tasks per node.
To express this in slurm we wrote as:

aprun -n 48 -N 24 ./code_mpi.x

We additionally have to avoid any inadvertent OpenMP threading by setting OMP_NUM_THREADS to 1:

export OMP_NUM_THREADS=1

The aprun changes when we have to use 2 partially occupied nodes , in this case there is a distribution of the 24MPI tasks to 2 nodes, each supplying 64GB and each running 12 MPI tasks.

aprun -n 24 -N 12 -S 6 ./code_mpi.x

As shown in the aprun, for 2 partially occupied nodes we also put 6 tasks per socket.
Everything else does not change from the fully occupied nodes except of the aprun to launch the job.


\Subsubsection{OMP Examples}







\Subsubsection{CUDA Examples}



\Subsubsection(Hybrid Examples}


\Subsection{Zeus}

The getexample tool also provides examples for Zeus which is according to Pawsey system descriptions an SGI Linux cluster that is principally utilized 
for pre and post-preparing of data, extensive shared memory calculations and remote visualization work. It is diverse group with 30 nodes in different layouts. It shares the same /home,/scratch and 
/group file systems with Magnus. To access all the 30 nodes, the client has to ssh to zeus.pawsey.org.au.
Zeus contains one large-memory nodes known as Zythos, which is SGI UV2000 system. Zythos contains 24 UV blades which show up as a solitary framework 
with large amounts of shared memory of 6TB.Out of 24 blades, 20 of them contain two 6-core Intel Xeon E5-4610 CPUs and the remnant contain only one of 
these CPUs and one other Nvidia Tesla K20 GPU. Numerous clients are allowed to keep running on Zythos simultaneously. Aside from Zythos, the other 29 
nodes of Zeus all have two 8-core Intel Xeon E5-2670 CPUs. GPUs are also featured in 27 of these with the 20 containing each one of them Nvidia Tesla 
K20 GPU. Only one user is allowed to work on each of the nodes at a time. Zeus is part of the arrangement of Pawsey Project assets and it is located at 
the Pawsey Supercomputing Center and it is incorporated with the rest of the infrastructure which permits a different scope of work.
 


\Subsubsection{MPI Jobs}

The getexample tool also includes MPI examples for Zeus with GNU, Intel and PGI compilers because both Zeus and Magnus have different operating systems 
and hence, the same MPI example needs to handled differently on Zeus. It is important to notice that the MPI source codes used for Zeus are the same
as Magnus, but the commands to run the same source codes are different on Zeus as compared to Magnus. Additionally, when the same code is run 
with different compilers such as GNU, Intel and PGI, the compiler module for the required compiler must be loaded as Magnus. This information was
included in the README and SLURM files of each example within the getexample tool and the required module for the specific compiler was already loaded 
within these files so that the user is not asked to download them manually to prevent any mistakes while running the code.

The two common source codes used for the MPI examples were hello_mpi.f90 (a FORTRAN 90 source code) and hello_mpi.c (a c code). These codes were used 
with GNU, Intel and PGI compiler options on Zeus to assist the users on how to run the same code on Zeus with different modules, even if the command 
for compiling the same source code does not for MPI tasks on Zeus.

As mentioned earlier, both codes were run on both Magnus and Zeus on 2 nodes. However, the partition in the SLURM was changed to workq from debugq to 
workq and to run the JOB on Zeus as shown:

#SBATCH --partition=workq
#SBATCH --nodes=2

Unlike Magnus, when --export=NONE is included in the SLURM for Zeus, the code does not compile and gives many errors. For this reason, this line was
excluded in the SLURM files of Zeus to compile the codes correctly.

To run the code with a given executable on scratch, and store the results in a directory within the GROUP directory to an output file, the generic 
variables were created in the SLURM as shown:

EXECUTABLE=hello_mpi_gnu
SCRATCH=$MYSCRATCH/run_hostname/$SLURM_JOBID
RESULTS=$MYGROUP/hellompi_gnu_results_zeus/$SLURM_JOBID

OUTPUT=hello_mpi_gnu.log

On Magnus, a total of 48 MPI tasks could be run whereas this was too much for Zeus and thus, the number of MPI tasks was reduced to 32. To run the job
on Zeus, instead of using aprun command used in Magnus, srun command was used as srun is specific to Zeus while aprun is designed for Magnus. Therefore,
to run the code, the following command was used as:

srun -n 32 -N 2 --mpi=pmi2 ./$EXECUTABLE >> ${OUTPUT}

where -n defines the total number of MPI tasks while -N defines the number of used on Zeus which is different from Magnus as -N defines the number of
MPI tasks per node. Once again, in comparison to Magnus --mpi=pmi2 was added to srun which wasn't used in aprun where --mpi=pmi2 is the MPI
implementation and must specified to srun for the correct operation.

In the README file, depending on which compiler the source code would be compiled with, the compiler module was included and loaded in the README file 
when it ran executed. For example, if the GNU compiler module was to be used, the gcc module was loaded with the mpt module as shown below:

module load gcc
module load mpt

If the code was to be compiled with Intel, the gcc module needs to unloaded and the Intel compiler module needs to be loaded. This applies to the PGI
compiler as well, after the gcc module is unloaded, the pgi module should be loaded as shown below:

For Intel:
module unload gcc
module load intel
module load mpt

For PGI:
module unload gcc
module load pgi
module load mpt

The mpt module needs to be loaded whenever an MPI or OpenMP/MPI task is submitted to Zeus as it provides the SGI message to access the MPI library.

Additionally, the README file contains the command to compile the FORTRAN 90, hello_mpi.f90 code  and hello_mpi.c c code with the GNU or Intel compiler 
as shown:

mpif90 hello_mpi.f90 -o hello_mpi_gnu
mpicc hello_mpi.c -o hello_mpi_gnu

For MPI tasks as mentioned earlier, the command to compile the code does not change for GNU and Intel compilers, whereas it differs for PGI compiler.
Hence to compile the basic hello_mpi.f90 and hello_mpi.c code, the following commands were used:

pgf90 -Mmpi=sgimpi hello_mpi.f90 -o hello_mpi_pgi
pgcc -Mmpi=sgimpi hello_mpi.c -o hello_mpi_c 
  






\Subsubsection{OMP Examples}






\Subsubsection{Cuda Examples}



\Subsubsection{Hybrid Examples}


\Subsection{Zythos}



\Subsubsection{MPI Examples}



\Subsubsection{OMP Examples}




\Subsubsection{Cuda Examples}



\Subsubsection{Hybrid Examples}


\Section{Other Completed Tasks}

\Subsection{Intel Xeon Processor}

\Subsection{MIC &MKL}


\Susection{Ubuntu}



\Subsection{Supercomputing Examples to Others}



\Section{Conclusion}




\Section{References}
\end{Document}
% end document
